{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e36429d",
   "metadata": {
    "id": "3e36429d"
   },
   "source": [
    "# AssurPrime : Saurez-vous prédire la prime d'assurance ?\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://challengedata.ens.fr/logo/public/CA_assurances_RVB_sans-raison-d%C3%AAtre_%C3%A9v%C3%A9nementiel_Mg5WVL6.png\" width=\"350\" title=\"Crédit Agricole Assurances\">\n",
    "  <p align=\"center\">\n",
    "    <a href=\"https://colab.research.google.com/github/auduvignac/challengedata_ens_AssurPrime/blob/main/notebooks/exploration/assurprim_modeling_workflow.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Google Colab\"/>\n",
    "    </a>\n",
    "  </p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a2dfa",
   "metadata": {},
   "source": [
    "# Présentation complète du projet\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Crédit Agricole Assurances est une filiale du Groupe Crédit Agricole dédiée à l’assurance, faisant de celui-ci un acteur multi-expert de la bancassurance et le 1er bancassureur en Europe.\n",
    "Crédit Agricole Assurances regroupe plusieurs entités, dont Predica et Pacifica, qui proposent une large gamme d’assurances aux particuliers, aux exploitants agricoles, aux professionnels et aux entreprises. Crédit Agricole Assurances s’engage à offrir des solutions innovantes et adaptées aux besoins des clients, tout en favorisant le développement durable et la responsabilité sociale.\n",
    "Au sein de l’Académie Data Science du groupe, l’objectif est de participer activement à la montée en compétences des collaborateurs, de partager des connaissances et d’identifier de nouveaux usages.\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Le contrat Multirisque Agricole, géré par Pacifica, est souscrit par les agriculteurs pour sécuriser leur exploitation. Il couvre l’activité professionnelle, les dommages aux bâtiments d’exploitation, le matériel stocké, ainsi que la protection financière et juridique. Ce contrat garantit à l’assuré une couverture efficace et durable, assurant ainsi la continuité de son activité en cas de sinistre, tant sur le plan matériel que financier.\n",
    "\n",
    "Actuellement, le risque d’incendie constitue une part majeure de la charge sinistre du contrat Multirisque Agricole, ce qui en fait un enjeu clef à modéliser avec précision.\n",
    "\n",
    "L’objectif est d’identifier le meilleur modèle pour prédire la prime pure incendie, en utilisant :\n",
    "- Un modèle pour la Fréquence,\n",
    "- Un modèle pour le Coût moyen.\n",
    "\n",
    "La variable cible finale, la charge, est obtenue en multipliant la fréquence, le coût moyen, et le nombre d’années depuis la souscription du contrat (la variable `ANNEE_ASSURANCE`).\n",
    "\n",
    "\n",
    "## Description des données\n",
    "\n",
    "Un fichier supplémentaire est mis à disposition regroupant toutes les variables disponibles, accompagnées de leur description. Ce fichier inclut :\n",
    "\n",
    "- **Les variables cibles** : `FREQ`, `CM`, et `CHARGE`\n",
    "- **Données géographiques** : département, données météorologiques, etc.\n",
    "- **Données spécifiques au contrat**, notamment :\n",
    "    - L’activité de l’assuré (cultivateur, polyculteur, etc.)\n",
    "    - Les indicateurs de souscription des garanties\n",
    "    - Le nombre de bâtiments, de salariés, et de sinistres déclarés lors de la souscription\n",
    "    - **Données de surface** : surfaces des bâtiments (élevage, exploitation, etc.), anonymisées en `surface1`, `surface2`, etc., pour garantir la confidentialité\n",
    "    - **Données de capitaux** : capitaux assurés pour différentes options (vol, serres, etc.), anonymisés en `capital1`, `capital2`, etc.\n",
    "    - **Données liées à la prévention** : présence d’équipements (extincteurs, structure en bois, etc.), anonymisées en `prev1`, `prev2`, etc.\n",
    "\n",
    "\n",
    "## Description du *benchmark*\n",
    "\n",
    "### Objectif du challenge\n",
    "\n",
    "L’objectif de ce challenge est de comparer les performances des modèles développés dans le cadre de cette compétition avec celles d’un modèle de référence basé sur des **GLM (Generalized Linear Models)** classiques.\n",
    "\n",
    "### Structure du benchmark\n",
    "\n",
    "Le benchmark repose sur deux modèles **GLM** distincts :\n",
    "\n",
    "- Fréquence des sinistres :\n",
    "    - Distribution : *Loi de Poisson* ;\n",
    "    - Fonction de lien : *Log*.\n",
    "\n",
    "- Coût moyen d’un sinistre :\n",
    "    - Distribution : *Tweedie* ;\n",
    "    - Fonction de lien : *Log*.\n",
    "\n",
    "### Évaluation\n",
    "\n",
    "L’évaluation des modèles repose sur une métrique unique : **RMSE (Root Mean Square Error)**, définie par la formule suivante :\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "où :\n",
    "- $y_i$ représente la valeur réelle ;\n",
    "- $\\hat{y}_i$ représente la valeur prédite ;\n",
    "- $n$ est le nombre d’observations.\n",
    "\n",
    "L’idée reste d’évaluer dans quelle mesure les approches proposées permettent de dépasser les performances des modèles standards tout en prenant en compte :\n",
    "\n",
    "- La précision des prédictions ;\n",
    "- Les aspects d’interprétabilité et d’efficacité ;\n",
    "- Les contraintes métier associées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54964240",
   "metadata": {},
   "source": [
    "# Résumé\n",
    "\n",
    "Le but est de construire un pipeline de modélisation permettant d’estimer la **charge sinistre incendie**, via deux composantes principales :\n",
    "\n",
    "- La **fréquence des sinistres** (`FREQ`)\n",
    "- Le **coût moyen des sinistres** (`CM`)\n",
    "\n",
    "La variable cible finale est définie comme : `CHARGE = FREQ × CM × ANNEE_ASSURANCE`.\n",
    "\n",
    "## Données disponibles\n",
    "\n",
    "- **Cibles** :\n",
    "  - `FREQ`;\n",
    "  - `CM`;\n",
    "  - `CHARGE`.\n",
    "- **Informations géographiques** :\n",
    "  - département;\n",
    "  - météo.\n",
    "- **Descripteurs du contrat** :\n",
    "  - activité assurée;\n",
    "  - garanties souscrites;\n",
    "  - nombre de bâtiments;\n",
    "  - surface;\n",
    "  - capitaux;\n",
    "  - équipements de prévention.\n",
    "- **Variables anonymisées** : certaines variables sensibles ont été anonymisées pour des raisons de confidentialité. Bien que leur signification exacte soit masquée, leur typologie (quantitative, binaire, etc.) permet d’en exploiter le contenu via des méthodes d’analyse statistique et de modélisation. Elles se présentent sous forme de groupes de variables thématiques telles que :\n",
    "  - `surface1`, `surface2`, ... : indicateurs de **surfaces agricoles** (bâtiments d’élevage, stockage, exploitation, etc.)\n",
    "  - `capital1`, `capital2`, ... : **capitaux assurés** pour différentes garanties optionnelles (ex. vol, serres, machines)\n",
    "  - `prev1`, `prev2`, ... : **mesures de prévention** contre les incendies ou sinistres (présence d’extincteurs, structures en matériaux spécifiques, etc.)\n",
    "\n",
    "\n",
    "## Modèle de référence (benchmark)\n",
    "\n",
    "Un double modèle **GLM** sert de base comparative :\n",
    "\n",
    "| Modèle        | Distribution | Fonction de lien |\n",
    "|---------------|--------------|------------------|\n",
    "| Fréquence     | Poisson      | Log              |\n",
    "| Coût moyen    | Tweedie      | Log              |\n",
    "\n",
    "Ces modèles sont évalués selon la **Root Mean Square Error (RMSE)** :$\\sqrt{\\frac{1}{n} \\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2}$, sur la variable finale `CHARGE`.\n",
    "\n",
    "## Contenu du notebook\n",
    "\n",
    "Ce notebook comprend les étapes suivantes :\n",
    "\n",
    "1. **Exploration des données**\n",
    "2. **Préparation des variables** (feature engineering, nettoyage, encodage)\n",
    "3. **Modélisation fréquence & coût moyen**\n",
    "4. **Évaluation des performances**\n",
    "5. **Analyse des résidus et interprétation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c534d",
   "metadata": {},
   "source": [
    "# Bibliothèques utilisées\n",
    "\n",
    "Cette section regroupe l’installation et l’importation des bibliothèques nécessaires pour l’ensemble du pipeline : de l’analyse exploratoire à la modélisation, en passant par le prétraitement des données et l’évaluation des performances.\n",
    "\n",
    "### Manipulation de données\n",
    "- `pandas`, `numpy` : gestion et transformation de données tabulaires et numériques.\n",
    "\n",
    "### Visualisation\n",
    "- `matplotlib.pyplot`, `seaborn` : création de graphiques pour l’exploration visuelle des données et l’interprétation des résultats.\n",
    "\n",
    "### Statistiques et tests\n",
    "- `scipy.stats.chi2_contingency` : test d’indépendance du khi² sur des variables qualitatives.\n",
    "\n",
    "### Machine Learning\n",
    "- `scikit-learn` : \n",
    "  - Prétraitement : `SimpleImputer`, `StandardScaler`\n",
    "  - Modélisation : régression logistique, Ridge, Random Forest, Gradient Boosting\n",
    "  - Validation croisée, tuning et métriques : RMSE, F1-score, AUC, etc.\n",
    "- `xgboost` : algorithmes de boosting performants via `XGBClassifier` et `XGBRegressor`.\n",
    "\n",
    "### Encodage des variables catégorielles\n",
    "- `category_encoders.CountEncoder` : encodage des modalités par fréquence d’apparition.\n",
    "\n",
    "### Affichage dans le notebook\n",
    "- `IPython.display.display` : affichage ciblé et lisible des DataFrames ou objets complexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61211875",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61211875",
    "outputId": "4589ac86-5bfa-4135-ab13-ef1efac4e734"
   },
   "outputs": [],
   "source": [
    "!pip install -q category-encoders \\\n",
    "                matplotlib \\\n",
    "                numpy \\\n",
    "                pandas \\\n",
    "                scikit-learn \\\n",
    "                seaborn \\\n",
    "                xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64730ff",
   "metadata": {
    "id": "d64730ff"
   },
   "outputs": [],
   "source": [
    "# Standard library (ex. pathlib, math, os, sys)\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party libraries (ex. numpy, pandas, matplotlib, scikit-learn)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from category_encoders import CountEncoder\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe7adf",
   "metadata": {
    "id": "8efe7adf"
   },
   "source": [
    "# Chargement et aperçu des données\n",
    "\n",
    "Chargement des fichiers de données d'entraînement à partir des sources distantes. Les fichiers comprennent :\n",
    "\n",
    "- `X_train` : variables explicatives issues des contrats d’assurance ;\n",
    "- `y_train` : variables cibles associées :\n",
    "  - `FREQ` : fréquence des sinistres incendie ;\n",
    "  - `CM` : coût moyen par sinistre ;\n",
    "  - `CHARGE` : charge totale sinistre incendie (`CHARGE = FREQ × CM × ANNEE_ASSURANCE`).\n",
    "\n",
    "Le chargement des fichiers repose sur une stratégie de *fallback* : les données sont d’abord recherchées en local (chemins absolus), et en cas d’absence, elles sont automatiquement téléchargées depuis des URLs distantes, puis conservées localement pour les exécutions futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des chemins locaux\n",
    "local_x_path = Path(\"../../data/raw/x_train.csv\").resolve()\n",
    "local_y_path = Path(\"../../data/raw/y_train.csv\").resolve()\n",
    "\n",
    "# URLs distantes (GitHub)\n",
    "X_train_url = (\n",
    "    \"https://media.githubusercontent.com/media/auduvignac/\"\n",
    "    \"challengedata_ens_AssurPrime/refs/heads/main/data/raw/x_train.csv\"\n",
    ")\n",
    "y_train_url = (\n",
    "    \"https://media.githubusercontent.com/media/auduvignac/\"\n",
    "    \"challengedata_ens_AssurPrime/refs/heads/main/data/raw/y_train.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_dataset(local_path, url, name):\n",
    "    \"\"\"\n",
    "    Charge un fichier CSV depuis un chemin local, ou le télécharge depuis\n",
    "    une URL si le fichier n'existe pas localement.\n",
    "\n",
    "    Le fichier est ensuite sauvegardé localement pour les exécutions\n",
    "    futures (fallback).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    local_path : pathlib.Path\n",
    "        Chemin vers le fichier local.\n",
    "    url : str\n",
    "        URL distante du fichier CSV.\n",
    "    name : str\n",
    "        Nom affiché lors du chargement (utilisé pour les logs).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Le contenu du fichier CSV sous forme de DataFrame.\n",
    "    \"\"\"\n",
    "    if local_path.exists():\n",
    "        print(f\"{name} chargé depuis : {local_path}\")\n",
    "        return pd.read_csv(local_path)\n",
    "    else:\n",
    "        print(f\"{name} non trouvé localement, téléchargement depuis l'URL...\")\n",
    "        df = pd.read_csv(url)\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(local_path, index=False)\n",
    "        print(\n",
    "            f\"{name} téléchargé et enregistré localement dans : {local_path}\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "\n",
    "# Chargement des jeux de données\n",
    "try:\n",
    "    print(\"Chargement des données...\")\n",
    "    X_train = load_dataset(local_x_path, X_train_url, \"X_train\")\n",
    "    y_train = load_dataset(local_y_path, y_train_url, \"y_train\")\n",
    "    print(\"Données chargées avec succès.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Échec du chargement des données.\")\n",
    "    print(\"Vérifiez la validité des URL ou la connexion internet.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258931d8",
   "metadata": {},
   "source": [
    "Une fois le chargement des données effectué, il convient tout d’abord de vérifier que `X_train` et `y_train` possèdent le même nombre de lignes. Un aperçu des premières lignes du jeu d’entraînement est ensuite présenté afin d’en examiner la structure et le contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimensions de X_train : {X_train.shape}\")\n",
    "print(f\"Dimensions de y_train : {y_train.shape}\")\n",
    "\n",
    "if len(X_train) == len(y_train):\n",
    "    print(\"Les jeux de données disposent bien du même nombre d'observations.\")\n",
    "else:\n",
    "    print(\n",
    "        \"Les jeux de données ne disposent pas du même nombre d'observations.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b904e60",
   "metadata": {},
   "source": [
    "La correspondance en nombre d’observations entre `X_train` et `y_train` étant assurée, nous poursuivons avec une inspection des premières colonnes de `X_train`, afin d’identifier la nature et l’hétérogénéité des variables disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d658668",
   "metadata": {},
   "source": [
    "## Aperçu des variables explicatives (`X_train`)\n",
    "\n",
    "Le jeu de données `X_train` contient les **variables explicatives** issues des contrats d’assurance souscrits dans le cadre du produit Multirisque Agricole.  \n",
    "Chaque ligne correspond à **un contrat**, et chaque colonne décrit une caractéristique associée.\n",
    "\n",
    "Ci-dessous, les premières lignes de `X_train` sont affichées pour donner un aperçu de la structure des données. disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aperçu de X_train\n",
    "print(\"Aperçu de X_train :\")\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b14543",
   "metadata": {},
   "source": [
    "Les premières colonnes mettent en évidence la richesse du jeu de données :\n",
    "\n",
    "- **Variables catégorielles** liées au contrat : `ACTIVIT2`, `VOCATION`, `TYPERS`, etc.\n",
    "- **Caractéristiques techniques ou déclaratives** : `CARACT1`, `CARACT2`, etc.\n",
    "- **Données météorologiques et historiques** : variables commençant par `NBJRR`, `RR_VOR`, `RRAB_VOR`, etc.\n",
    "- **Variables numériques continues** comme `ANNEE_ASSURANCE`, indiquant l’ancienneté du contrat.\n",
    "\n",
    "Certaines colonnes contiennent :\n",
    "\n",
    "- Des **valeurs manquantes** qu’il faudra prendre en compte dans le cadre du prétraitement ;\n",
    "- Des données **anonymisées ou encodées** (`surface1`, `capital1`, `prev1`, etc.) pour des raisons de confidentialité.\n",
    "\n",
    "### Statistiques descriptives de `X_train`\n",
    "\n",
    "Examinons les statistiques descriptives de `X_train` pour mieux comprendre la distribution des variables, identifier les valeurs manquantes et détecter les éventuelles anomalies.\n",
    "\n",
    "Pour cela, une méthode `scrollable_describe` a été implémentée. Elle s’appuie sur la méthode `describe()` de `pandas` pour générer un résumé statistique des colonnes numériques du DataFrame, tout en encapsulant l’affichage dans un bloc défilant afin de mutualiser les appels et et assurer un lisibilité plus aisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrollable_describe(df, height=400, round=2):\n",
    "    \"\"\"\n",
    "    Affiche une version transposée et scrollable du résumé statistique\n",
    "    d'un DataFrame (résultat de .describe()).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        jeu de données à décrire\n",
    "    height : int\n",
    "        hauteur de la boîte scrollable en pixels\n",
    "    round : int\n",
    "        nombre de décimales pour l'arrondi des valeurs\n",
    "    Returns\n",
    "    -------\n",
    "    HTML : IPython.display.HTML\n",
    "        une représentation HTML scrollable du résumé statistique\n",
    "    \"\"\"\n",
    "    desc = df.describe().T.round(round)\n",
    "    html = desc.to_html()\n",
    "    return HTML(\n",
    "        f\"\"\"\n",
    "        <div style=\"height:{height}px; overflow:auto;\n",
    "                    border:1px solid lightgray; padding:10px\">\n",
    "            {html}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e9a07",
   "metadata": {},
   "source": [
    "Le tableau ci-dessous fournit un résumé statistique des **94 variables numériques** de `X_train`, calculé sur les colonnes de types `int64` et `float64`.\n",
    "\n",
    "Ce résumé permet notamment de :\n",
    "- Identifier les **ordres de grandeur** et la **variabilité** des variables ;\n",
    "- Détecter la présence de **valeurs manquantes** (via les valeurs de `count`) ;\n",
    "- Repérer les **variables constantes** ou à faible dispersion. Pour cela, l’écart-type (*standard deviation*), permettant de mesurer à quel point les valeurs d’une variable sont dispersées autour de la moyenne, sera particulièrement utile. Plus l'écart-type est élevé, plus la variabilité est élevée (i.e. les valeurs sont dispersées), ce qui peut laisser supposer que la variable apporte une information discriminante pour la modélisation. À l’inverse, un écart-type proche de 0 indique une faible variabilité, ce qui peut suggérer que la variable n’apporte pas d’information discriminante pour la modélisation ;\n",
    "- Observer les distributions à travers les **quartiles** (`25%`, `50%`, `75%`) ;\n",
    "- Identifier d’éventuelles **valeurs extrêmes** (`min`, `max`) pouvant signaler des outliers.\n",
    "\n",
    "L’analyse de ce tableau permettra de prioriser les traitements à venir (imputation, transformation, normalisation, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrollable_describe(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf46db",
   "metadata": {},
   "source": [
    "### Premiers constats quant aux statistiques descriptives relatives à `X_train`\n",
    "\n",
    "- Plusieurs variables comme `CARACT2`, `RISK5` ou `ZONE_VENT` affichent un nombre d’observations inférieur au nombres de lignes associés au *dataset* (`count = 383 610`), indiquant la présence de **valeurs manquantes** (déjà constatée dans l’aperçu des données) ;\n",
    "- Certaines variables ont des **valeurs maximales très élevées** (`CA1`, `CA2` jusqu’à `30 000`, `CA3` jusqu’à `50 000`), ce qui pourrait **influencer fortement la distribution**, notamment en cas de forte asymétrie ou de valeurs extrêmes isolées (*outliers*) ;\n",
    "- Des colonnes comme `EQUIPEMENT1` ou `EQUIPEMENT3` semblent **binaires ou quasi-binaires** (`min = 0`, `max = 1`, `mean` faible), ce qui suggère qu'elles peuvent être traitées comme des **indicateurs logiques** ;\n",
    "- La colonne `ANNEE_ASSURANCE` varie entre environ `0.0027` et `2.0`, ce qui suggère qu’il s’agit d’un **ratio (ou prorata) d’années** plutôt qu’un entier représentant un nombre d’années complètes ;\n",
    "- Certaines variables telles que `DEROG15`, `TYPBAT2` ou `CARACT5` présentent une **variabilité très faible** (majorité de modalités identiques), ce qui peut **limiter leur pouvoir discriminant** en modélisation et justifie une réflexion sur leur maintien ;\n",
    "- D'autres variables comme `KAPITAL10`, `KAPITAL11` ou `KAPITAL12` montrent une grande **hétérogénéité en valeur**, parfois avec une majorité de zéros, ce qui pourrait indiquer une **sparsité partielle** qu'il conviendra d’examiner plus précisément ;\n",
    "- Enfin, certaines familles de variables (`SURFACE`, `NBBAT`, `KAPITAL`, `RISK`) présentent des structures répétées (suffixes numériques), suggérant un **modèle de codage hiérarchique ou multi-dimensionnel**, qu’il pourrait être utile de regrouper ou résumer par des agrégats statistiques lors du prétraitement.\n",
    "\n",
    "Cette analyse guidera la **phase de nettoyage** et de **sélection de variables**, en complément de la visualisation et du traitement des valeurs manquantes.\n",
    "\n",
    "Dans une démarche analogue, examinons à présent la structure de `y_train`, qui regroupe les **variables cibles** indispensables à la modélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207ef69",
   "metadata": {},
   "source": [
    "## Aperçu des variables cibles (`y_train`)\n",
    "\n",
    "Le jeu de données `y_train` contient les **variables cibles** associées à chaque contrat :\n",
    "\n",
    "- `FREQ` : fréquence des sinistres liés à l’incendie (éventuellement nulle) ;\n",
    "- `CM` : coût moyen des sinistres (en euros) ;\n",
    "- `ANNEE_ASSURANCE` : nombre d’années depuis la souscription ;\n",
    "- `CHARGE` : charge sinistre totale (`CHARGE = FREQ × CM × ANNEE_ASSURANCE`)\n",
    "\n",
    "Un extrait de `y_train` est présenté ci-dessous permettant ainsi d'en observer la structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aperçu de y_train\n",
    "print(\"Aperçu de y_train :\")\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75034496",
   "metadata": {},
   "source": [
    "Un aperçu des premières lignes de `y_train` permet de visualiser la structure du jeu de données cible. Les variables `FREQ`, `CM` et `CHARGE` y sont systématiquement nulles pour les premières observations, ce qui reflète des cas sans sinistre déclaré. Ce constat préliminaire met en évidence un déséquilibre important dans la variable `CHARGE`, à prendre en compte lors de la modélisation. La variable `ANNEE_ASSURANCE`, exprimée sous forme décimale, suggère une granularité infra-annuelle.\n",
    "\n",
    "### Elements de précisions sur la variable `ANNEE_ASSURANCE`\n",
    "\n",
    "La variable `ANNEE_ASSURANCE` semble représenter le **temps écoulé depuis la souscription du contrat**, exprimé en **fraction d’année**. Par exemple, une valeur de `0.5` correspondrait à environ six mois d’ancienneté, tandis qu’une valeur de `1.0` indiquerait une année complète.\n",
    "\n",
    "Plus formellement, cette variable pourrait être interprétée comme un **rapport entre le nombre de jours écoulés depuis la souscription et le nombre de jours dans une année**, selon la formule suivante :\n",
    "\n",
    "$$\n",
    "\\text{ANNEE\\_ASSURANCE} = \\frac{\\text{Nombre de jours depuis la souscription}}{365}\n",
    "$$\n",
    "\n",
    "Cette granularité permet :\n",
    "- de modéliser plus finement les effets du temps d’exposition au risque dans un cadre assurantiel ;\n",
    "- d’éviter les pertes d’information liées à un arrondi à l’unité (par exemple en années entières).\n",
    "\n",
    "Dans les modèles assurantiels, on modélise souvent la durée d’exposition au risque sous forme fractionnaire (i.e. en années décimales), notamment pour :\n",
    "- Modéliser les sinistres en fréquence :\n",
    "La fréquence des sinistres est souvent rapportée à une année. Si un contrat n’est actif que 6 mois, il est logique d’ajuster son poids dans le calcul :\n",
    "$$\n",
    "\\text{Fréquence annualisée} = \\frac{\\text{Nombre de sinistres}}{\\text{Durée d'exposition en années}}\n",
    "$$\n",
    "- Pondérer les charges sinistres :\n",
    "Dans une modélisation de charge sinistre, une observation ayant été exposée moins longtemps au risque doit logiquement avoir une influence proportionnelle à cette durée.\n",
    "- Utiliser des modèles adaptés : Poisson, Gamma ou encore Tweedie\n",
    "\n",
    "Dans la continuité de l’analyse effectuée sur `X_train`, étoffons le propos précédent avec les statistiques descriptives de `y_train`.\n",
    "\n",
    "### Statistiques descriptives de `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le nombre de variables présentes dans y_train étant plus faible que dans\n",
    "# X_train, on peut afficher le résumé statistique sans scroll et ainsi\n",
    "# affecter à height une valeur plus petite, en l'occurrence 160 pixels.\n",
    "scrollable_describe(y_train, height=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f813b",
   "metadata": {},
   "source": [
    "### Premiers constats quant aux statistiques descriptives relatives à `y_train`\n",
    "\n",
    "L’analyse statistique des variables cibles met en évidence plusieurs éléments majeurs à prendre en compte dans la suite du traitement :\n",
    "\n",
    "- La variable `FREQ`, représentant la fréquence de sinistres, présente une valeur moyenne extrêmement faible (`0.01`), ce qui reflète un déséquilibre important entre les contrats avec et sans sinistre. Une valeur maximale de `182.5` est observée, suggérant un cas atypique ou potentiellement erroné qui mérite une vérification approfondie.\n",
    "\n",
    "- La variable `CM` (coût moyen des sinistres), présente une distribution très étendue, avec une moyenne de `182.52`, un écart-type très élevé ($\\sigma = 6 699.97$), et des valeurs aberrantes notables allant jusqu’à `500 000`. La présence d'une valeur minimale négative (`-5 751.0`) est incohérente et indique probablement une erreur d'encodage ou un cas particulier.\n",
    "\n",
    "Les cas de figures pouvant exprimer un `CM` négatif sont les suivants :\n",
    "\n",
    "- **Recours ou subrogation** : remboursement par un tiers responsable d’un sinistre ;\n",
    "- **Correction comptable** ou **ajustement rétroactif** d’un sinistre ;\n",
    "- **Remboursement net** en faveur de l’assureur dans des cas exceptionnels ;\n",
    "- **Erreur d’encodage** ou convention spécifique de saisie.\n",
    "\n",
    "Ces explications demeurent spéculatives. Une clarification avec les équipes métiers ou une exploration plus approfondie du contexte des données serait nécessaire pour statuer sur la légitimité ou non de ces valeurs. Malheurseusement dans le cadre du projet aucune information complémentaire n'est fournie.\n",
    "\n",
    "Selon l’impact observé, une décision de nettoyage ou de transformation des données pourrait être envisagée lors du prétraitement.\n",
    "\n",
    "- La variable `CHARGE`, calculée comme le produit `FREQ × CM × ANNEE_ASSURANCE`, hérite logiquement de cette forte hétérogénéité. Elle présente une moyenne de `186.09`, mais également une grande dispersion ($ \\sigma \\approx 6 800$) et une valeur maximale de `552 000`, soulignant la nécessité d’un traitement robuste des valeurs extrêmes. Une valeur minimale négative est également observée, ce qui renforce l’idée de valeurs anormales dans `CM`.\n",
    "\n",
    "- Enfin, la variable `ANNEE_ASSURANCE` varie de `0.0` à `2.0`, ce qui indique vraisemblablement une représentation continue d’un ratio d’ancienneté ou d’année d’assurance, et non une valeur entière comme une simple année calendaire.\n",
    "\n",
    "Ces constats mettent en évidence la nécessité de traitements spécifiques lors de la phase de prétraitement, en particulier sur les valeurs extrêmes et aberrantes, ainsi que sur la rareté structurelle des sinistres, qui devra être prise en compte dans le choix des algorithmes, des métriques d’évaluation et des méthodes de validation.\n",
    "\n",
    "Pour étoffer l'analyse, vérifions dans quelle mesure ce déséquilibre est présent dans l’ensemble du jeu de données, en calculant le **nombre et le pourcentage de lignes** avec `FREQ = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dcedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre et du pourcentage de lignes avec FREQ nulle\n",
    "nb_freq_zero = (y_train[\"FREQ\"] == 0).sum()\n",
    "pourcentage_freq_zero = nb_freq_zero / len(y_train) * 100\n",
    "\n",
    "print(f\"Nombre de lignes avec FREQ = 0 : {nb_freq_zero}\")\n",
    "print(f\"Nombre de lignes totales : {len(y_train)}\")\n",
    "print(f\"Proportion de FREQ = 0 : {pourcentage_freq_zero:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0899b",
   "metadata": {},
   "source": [
    "Ce résultat met en évidence les éléments suivants :\n",
    "\n",
    "- Une **écrasante majorité** des contrats présentent une fréquence de sinistres nulle :\n",
    "  **380 716 lignes sur 383 610**, soit **99,25 %** des observations, ont `FREQ = 0`.  \n",
    "  Cela signifie que la variable `CHARGE` est également nulle dans la quasi-totalité des cas ;\n",
    "  \n",
    "- Ce **déséquilibre extrême**, typique des données assurantielles, où les sinistres sont rares mais coûteux, aura un impact important sur :\n",
    "  - Le choix des modèles (ex. classification binaire et régression) ;\n",
    "  - Les métriques d’évaluation (ex. RMSE pondérée, F1-score sur sinistrés) ;\n",
    "  - Les stratégies de sous-échantillonnage, sur-échantillonnage ou filtrage.\n",
    "\n",
    "## Synthèse générale de l’analyse exploratoire\n",
    "\n",
    "L’inspection de `X_train` et `y_train` a permis de dégager plusieurs éléments clefs qui orienteront les étapes de prétraitement et de modélisation :\n",
    "\n",
    "#### Concernant les variables explicatives (`X_train`)\n",
    "- Le jeu de données est riche et hétérogène, incluant des **variables catégorielles**, **numériques**, **techniques**, ainsi que des **données météorologiques et historiques** ;\n",
    "- De nombreuses **valeurs manquantes** sont présentes dans certaines colonnes, impliquant des stratégies d’imputation ciblées ;\n",
    "- Plusieurs variables présentent des **valeurs extrêmes** ou une **forte dispersion**, qui pourraient biaiser certains algorithmes sensibles à l’échelle des données (ex. régression linéaire, KNN) ;\n",
    "- Certaines colonnes montrent une **faible variabilité**, voire sont quasi constantes, posant la question de leur **pertinence prédictive** ;\n",
    "- Des motifs récurrents dans les noms de colonnes (ex. `KAPITAL`, `SURFACE`, `NBBAT`) laissent entrevoir des structures hiérarchiques ou redondantes, propices à une agrégation ou à une sélection ;\n",
    "- La variable `ANNEE_ASSURANCE` est exprimée en **fraction d’année**, élément pertinent pour modéliser l’exposition au risque mais qui nécessite une attention particulière lors de la modélisation.\n",
    "\n",
    "#### Concernant les variables cibles (`y_train`)\n",
    "- Le champ `CHARGE`, qui constitue la cible principale, est dérivé du produit `FREQ × CM × ANNEE_ASSURANCE`, ce qui en fait une **variable composite** ;\n",
    "- Un **déséquilibre extrême** est observé : plus de **99 %** des contrats présentent `FREQ = 0`, conduisant mécaniquement à `CHARGE = 0`. Cette rareté des sinistres est un défi classique en assurance, à traiter via des approches robustes ;\n",
    "- La variable `CM` (coût moyen des sinistres) présente des **valeurs anormales**, notamment des valeurs **négatives** ou **très élevées**, qui devront faire l’objet d’un traitement particulier (nettoyage ou transformation) ;\n",
    "- `ANNEE_ASSURANCE` semble être une **valeur continue** (et non entière), représentant un **ratio temporel** ou une part d’année d’exposition au risque.\n",
    "\n",
    "Ces constats initiaux soulignent la nécessité d’une analyse exploratoire approfondie, afin de mieux cerner la structure, la qualité et la distribution des données. Cette inspection permettra d’orienter les décisions à venir, notamment en matière de prétraitement rigoureux : gestion des valeurs manquantes, détection des anomalies, et adaptation des variables aux exigences des modèles prédictifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5e6ed",
   "metadata": {
    "id": "c7a5e6ed"
   },
   "source": [
    "# Analyse exploratoire approfondie des données et nettoyage progressif\n",
    "\n",
    "L’analyse exploratoire précédente a permis de mettre en évidence plusieurs caractéristiques structurelles du jeu de données.\n",
    "Pour compléter cette exploration, un approfondissement du jeu de données associé à un nettoyage progressif de `X_train` et `y_train` est entrepris.\n",
    "Cette démarche vise à affiner la compréhension des données, à identifier les variables pertinentes et à préparer le terrain pour les étapes de modélisation ultérieures.\n",
    "\n",
    "Le tableau ci-dessous résume les principales étapes de nettoyage et de préparation des données, ainsi que leurs objectifs respectifs :\n",
    "\n",
    "\n",
    "| Étape                                         | Objectif                                                |\n",
    "| --------------------------------------------- | ------------------------------------------------------- |\n",
    "| Suppression des variables non informatives | Cardinalité = 1, trop de valeurs manquantes             |\n",
    "| Harmonisation des types                    | Forcer les `int`, `float`, `category`, `bool`, etc.     |\n",
    "| Standardisation des modalités              | Supprimer les blancs, corriger les libellés incohérents |\n",
    "| Détection + marquage des valeurs atypiques | Par seuils, IQR ou z-score                              |\n",
    "| Réduction des modalités rares              | Grouper les catégories peu fréquentes                   |\n",
    "| Traitement des valeurs manquantes           | Imputation, remplacement ou exclusion raisonnée         |\n",
    "\n",
    "Préalablement, une fonction `scrollable_describe` est définie pour afficher les statistiques descriptives de manière défilante, facilitant ainsi la visualisation des résultats sur l’ensemble des variables.\n",
    "De plus, une copie de `X_train`, intitulée `X_clean`, est créée pour préserver l’intégrité des données originales tout au long du processus de nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ef4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scrollable_df(\n",
    "    df, height=300, border_color=\"lightgray\", padding=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Affiche un DataFrame dans une boîte HTML scrollable.\n",
    "\n",
    "    Paramètres :\n",
    "    - df : DataFrame à afficher\n",
    "    - height : hauteur (en pixels) de la boîte (par défaut 300)\n",
    "    - border_color : couleur de la bordure (par défaut lightgray)\n",
    "    - padding : espacement intérieur (en pixels, par défaut 10)\n",
    "    \"\"\"\n",
    "    html = df.to_html()\n",
    "    return HTML(\n",
    "        f\"\"\"\n",
    "        <div style=\"height:{height}px; overflow:auto;\n",
    "                    border:1px solid {border_color}; padding:{padding}px\">\n",
    "            {html}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = X_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aea850",
   "metadata": {},
   "source": [
    "## Répartition des types de variables dans `X_train`\n",
    "\n",
    "L’inspection des types de données permet d’identifier la nature des variables présentes dans le jeu `X_train`.\n",
    "Cette étape est essentielle pour distinguer :\n",
    "\n",
    "- Les variables numériques (quantitatives) ;\n",
    "- Les variables catégorielles (qualitatives) ;\n",
    "- D’éventuelles valeurs mal typées, comme par exemple des colonnes numériques encodées en `object` à cause de formats hétérogènes.\n",
    "\n",
    "Cette classification est indispensable pour anticiper les traitements appropriés lors de la phase de prétraitement : encodage, imputation, normalisation, etc.\n",
    "\n",
    "Affichons les types de données de `X_train` ainsi que le nombre de colonnes associées à chaque type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b29b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61834dd",
   "metadata": {},
   "source": [
    "La répartition des types de variables dans `X_train` est la suivante :\n",
    "\n",
    "- 280 variables de type `object` : ces colonnes correspondent à des données catégorielles ou textuelles. Elles nécessitent un traitement spécifique (encodage, nettoyage) avant utilisation dans un modèle ;\n",
    "- 58 variables de type `int64` : généralement des entiers utilisés comme indicateurs, compteurs ou encodage de modalités ;\n",
    "- 36 variables de type `float64` : variables numériques continues, souvent associées à des mesures (ex. : surface, capitaux, indices).\n",
    "\n",
    "Après avoir identifié la nature des variables dans `X_train`, une première étape de nettoyage consiste à supprimer les variables non informatives.\n",
    "Ces variables, sans réelle valeur ajoutée pour l'apprentissage, peuvent altérer la performance des modèles ou en complexifier inutilement l'entraînement.\n",
    "\n",
    "Nous procéderons selon les critères suivants, dans cet ordre :\n",
    "- Colonnes quasi vides : contenant un taux élevé de valeurs manquantes, rendant leur interprétation incertaine ;\n",
    "- Colonnes constantes : ne présentant qu’une seule modalité, et donc aucune variabilité ;\n",
    "- Colonnes déséquilibrées : présentant une modalité dominante écrasante (ex. > 99 %) ;\n",
    "- Colonnes identifiantes : contenant des identifiants uniques (ex. ID) sans relation directe avec la variable cible.\n",
    "\n",
    "Cette approche graduelle permet de rationaliser le jeu de données tout en préservant l’information pertinente pour la modélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872b707",
   "metadata": {},
   "source": [
    "## Détection et suppression des variables non informatives\n",
    "\n",
    "### Analyse des valeurs manquantes\n",
    "\n",
    "L’analyse des valeurs manquantes permet d’évaluer la qualité du jeu de données en identifiant les variables partiellement renseignées. Ces colonnes devront faire l’objet d’un traitement spécifique : suppression, imputation ou méthodes adaptées au contexte.\n",
    "\n",
    "Pour chaque variable, seront indiqués :\n",
    "\n",
    "- Le **nombre de valeurs manquantes** ;\n",
    "- La **proportion de valeurs manquantes** par rapport au total d’observations ;\n",
    "- Un **tri décroissant** selon cette proportion, afin de faire ressortir les variables les plus concernées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre et taux (%) de valeurs manquantes\n",
    "nb_missing = X_train.isnull().sum()\n",
    "taux_missing = X_train.isnull().mean() * 100\n",
    "\n",
    "# DataFrame résumé\n",
    "missing_info = pd.DataFrame(\n",
    "    {\n",
    "        \"Variables\": nb_missing.index,\n",
    "        \"Valeurs manquantes\": nb_missing,\n",
    "        \"Taux (%)\": taux_missing.round(2),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Filtrage des colonnes avec au moins une valeur manquante\n",
    "missing_info = missing_info[missing_info[\"Valeurs manquantes\"] > 0]\n",
    "\n",
    "# Tri décroissant par taux\n",
    "missing_info = missing_info.sort_values(by=\"Taux (%)\", ascending=False)\n",
    "\n",
    "display_scrollable_df(missing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d1cf2",
   "metadata": {},
   "source": [
    "Le tableau obtenu met en évidence une proportion importante de variables avec des taux de valeurs manquantes significatifs. Ces variables peuvent être classées selon différents niveaux d’impact :\n",
    "\n",
    "- **Totalement ou quasi totalement manquantes** : `DEROG14` (100 %), `DEROG13` (99,66 %), `DEROG16` (99,15 %)\n",
    "- **Taux très élevés (> 90 %)** : `CARACT2`, `CARACT3`, `TYPBAT1`, `DEROG12`\n",
    "- **Taux intermédiaires (56–62 %)** : variables météo et distances (`RRAB_VOR_MM_A`, `DISTANCE_311`, etc.)\n",
    "- **Taux modérés (20–40 %)** : `RISK13`, `KAPITAL11`, `SURFACE8`, etc.\n",
    "- **Taux faibles (< 10 %)** : `MEN_MAIS`, `RISK1`, `FRCH2`, etc.\n",
    "\n",
    "> **Remarques méthodologiques :**\n",
    "> - Les variables affichant plus de **90 % de valeurs manquantes** sont de sérieuses candidates à l’exclusion.\n",
    "> - Certaines **familles de variables cohérentes** (distances, météo) justifient une réflexion approfondie :\n",
    ">   - Imputation croisée entre variables similaires ;\n",
    ">   - Imputation conditionnelle selon le contexte géographique ou sectoriel ;\n",
    ">   - Réduction de dimension (agrégation, ACP).\n",
    "\n",
    "#### Suppression des colonnes quasi vides (trop de valeurs manquantes)\n",
    "\n",
    "En se basant sur l’analyse précédente, les colonnes présentant plus de **90 % de valeurs manquantes** sont supprimées du jeu de données `X_train_clean`.\n",
    "Cette étape permet de réduire le bruit et la complexité du modèle en éliminant les variables qui n’apportent pas d’information significative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seuil de tolérance de valeurs manquantes (par exemple, 90 % de NaN)\n",
    "missing_thresh = 0.9\n",
    "\n",
    "# Calcul du pourcentage de valeurs manquantes\n",
    "missing_ratio = X_train_clean.isnull().mean()\n",
    "\n",
    "# Sélection des colonnes à supprimer\n",
    "cols_missing = missing_ratio[missing_ratio > missing_thresh].index.tolist()\n",
    "\n",
    "print(\n",
    "    f\"Les {len(cols_missing)} colonnes avec plus de {missing_thresh*100:.0f}% \"\n",
    "    f\"de valeurs manquantes ont été supprimées :{cols_missing}\"\n",
    ")\n",
    "X_train_clean.drop(columns=cols_missing, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b3b64",
   "metadata": {},
   "source": [
    "Affichons le nombre de colonnes restantes après cette première étape de nettoyage.\n",
    "Après la suppression des colonnes quasi vides, le jeu de données `X_train_clean` contient désormais **88 colonnes**. Cette réduction significative permet de se concentrer sur les variables les plus pertinentes pour la modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cab9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre et taux (%) de valeurs manquantes\n",
    "nb_missing = X_train_clean.isnull().sum()\n",
    "taux_missing = X_train_clean.isnull().mean() * 100\n",
    "\n",
    "# DataFrame résumé\n",
    "missing_info = pd.DataFrame(\n",
    "    {\n",
    "        \"Variables\": nb_missing.index,\n",
    "        \"Valeurs manquantes\": nb_missing,\n",
    "        \"Taux (%)\": taux_missing.round(2),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Filtrage des colonnes avec au moins une valeur manquante\n",
    "missing_info = missing_info[missing_info[\"Valeurs manquantes\"] > 0]\n",
    "\n",
    "# Tri décroissant par taux\n",
    "missing_info = missing_info.sort_values(by=\"Taux (%)\", ascending=False)\n",
    "\n",
    "display_scrollable_df(missing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac8661",
   "metadata": {},
   "source": [
    "La suppression des colonnes quasi vides réalisée, supprimons à présent les colonnes constantes, c’est-à-dire celles qui ne présentent qu’une seule modalité (valeur unique) sur l’ensemble des observations. Ces variables n’apportent aucune information discriminante pour la modélisation et peuvent être éliminées.\n",
    "\n",
    "#### Suppression des colonnes constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes avec une seule valeur unique\n",
    "cols_constant = [\n",
    "    col\n",
    "    for col in X_train_clean.columns\n",
    "    if X_train_clean[col].nunique(dropna=False) <= 1\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Les {len(cols_constant)} colonnes constantes ont été supprimées : \"\n",
    "    f\"{cols_constant}\"\n",
    ")\n",
    "X_train_clean.drop(columns=cols_constant, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae13c12",
   "metadata": {},
   "source": [
    "Aucune colonne constante n’a été détectée dans `X_train_clean`, ce qui est un bon signe. Cela signifie que toutes les variables restantes présentent une certaine variabilité, ce qui est essentiel pour la modélisation.\n",
    "Abordons à présent la question des variables déséquilibrées, c’est-à-dire celles qui présentent une modalité dominante écrasante.\n",
    "\n",
    "#### Colonnes avec une modalité ultra-dominante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seuil de dominance (ex. 99 % pour une seule modalité)\n",
    "dominance_thresh = 0.99\n",
    "cols_dominant = []\n",
    "\n",
    "for col in X_train_clean.columns:\n",
    "    top_freq = (\n",
    "        X_train_clean[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    )\n",
    "    if top_freq > dominance_thresh:\n",
    "        cols_dominant.append(col)\n",
    "\n",
    "print(\n",
    "    f\"Les {len(cols_dominant)} colonnes avec une modalité \"\n",
    "    f\"dominante > {dominance_thresh*100:.0f}% ont été supprimées : \"\n",
    "    f\"{cols_dominant}\"\n",
    ")\n",
    "X_train_clean.drop(columns=cols_dominant, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04102217",
   "metadata": {},
   "source": [
    "L'étape de suppression des colonnes avec une modalité ultra-dominante réalisée, supprimons à présente la colonnes relatives aux identifiants des données : `ID`.\n",
    "\n",
    "#### Suppression des colonnes identifiantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58824a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression de la colonne ID\n",
    "cols_id = [\"ID\"]\n",
    "print(f\"{len(cols_id)} colonne(s) identifiante(s) supprimée(s).\")\n",
    "X_train_clean.drop(columns=cols_id, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff091a5",
   "metadata": {},
   "source": [
    "Affichons de le nombre de colonnes restantes dans `X_train_clean` après cette étape de nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054446e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train_clean.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204edcec",
   "metadata": {},
   "source": [
    "Le nettoyage a permis de supprimer 14 colonnes `object`, 12 colonnes `int64` et 5 colonnes `float64` pour cause de vide, constance, dominance ou rôle d’identifiant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baade2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du DataFrame typé\n",
    "df_types_sorted = (\n",
    "    X_train.dtypes.to_frame(name=\"dtype\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"nom\"})\n",
    ")\n",
    "\n",
    "# Ajout du nombre de modalités uniques\n",
    "df_types_sorted[\"n_modalites\"] = df_types_sorted[\"nom\"].apply(\n",
    "    lambda col: X_train[col].nunique()\n",
    ")\n",
    "\n",
    "# Tri par type et nom\n",
    "df_types_sorted = df_types_sorted.sort_values(by=[\"dtype\", \"n_modalites\"])\n",
    "\n",
    "# Affichage\n",
    "display_scrollable_df(df_types_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c5f34",
   "metadata": {},
   "source": [
    "Maintenant que le nombre de modalités est connu pour chaque variable (et les types bien identifiés), la prochaine étape concerne le traitement des variables mal typées.\n",
    "\n",
    "#### Conversion des variables `object` à faible cardinalité en `category`\n",
    "\n",
    "Pour les variables de type `object` avec une cardinalité faible (moins de 50 modalités), il est pertinent de les convertir en type `category`.\n",
    "Cette conversion permet de réduire l'empreinte mémoire et d'améliorer les performances des algorithmes de machine learning, tout en conservant la nature catégorielle des données.\n",
    "En l'occurrence, le nombre de modalités le plus élevé pour les variables de type `object` est de 16, ainsi la conversion porte sur les 280 variables de type `object` de `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion directe de toutes les colonnes object en category\n",
    "obj_cols = X_train_clean.select_dtypes(include=\"object\").columns\n",
    "\n",
    "X_train_clean[obj_cols] = X_train_clean[obj_cols].astype(\"category\")\n",
    "\n",
    "print(f\"Les {len(obj_cols)} variables ont été converties en 'category'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_before = X_train[obj_cols].memory_usage(deep=True).sum() / 1024**2\n",
    "mem_after = X_train_clean[obj_cols].memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"Mémoire utilisée avant : {mem_before:.2f} Mo\")\n",
    "print(f\"Après conversion en 'category' : {mem_after:.2f} Mo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931c9cc",
   "metadata": {},
   "source": [
    "Toutes les variables typées en `object` présentaient un nombre de modalités uniques $\\leq 16$. Ce profil suggère qu'elles sont vraisemblablement de nature catégorielle plutôt que textuelle libre.\n",
    "\n",
    "Afin d'optimiser la mémoire et de faciliter l'encodage ultérieur, ces variables ont été converties en `category`. Cette conversion a permis une **réduction significative de la mémoire utilisée** :\n",
    "\n",
    "- **Avant conversion** : 5 271,69 Mo  \n",
    "- **Après conversion** : 102,56 Mo  \n",
    "\n",
    "Cette étape constitue une **optimisation précieuse** dans le cadre d’un pipeline de traitement efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161af5a",
   "metadata": {},
   "source": [
    "#### Conversion en `numeric` des variables `object` mal encodées\n",
    "\n",
    "Certaines variables de type `object` peuvent contenir des valeurs numériques mal encodées (par exemple, des chaînes de caractères représentant des nombres).\n",
    "Pour ces variables, une conversion en type `numeric` est nécessaire.\n",
    "Cette conversion permet de traiter correctement les données numériques et d'éviter les erreurs lors des calculs statistiques ou des modélisations.\n",
    "Nous allons tenter une conversion automatique de ces variables en `float64` ou `int64`, selon leur contenu et ensuite repérer les erreurs potentielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7fc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple pour détecter les variables numériques mal typées\n",
    "possible_numeric = df_types_sorted.query(\n",
    "    \"dtype == 'object' and n_modalités > @max_modalites_category\"\n",
    ")[\"nom\"]\n",
    "\n",
    "# Tester la conversion et repérer celles qui échouent\n",
    "for col in possible_numeric:\n",
    "    try:\n",
    "        X_train_clean[col] = pd.to_numeric(X_train_clean[col], errors=\"raise\")\n",
    "        print(f\"{col} convertie avec succès.\")\n",
    "    except Exception as e:\n",
    "        print(f\"{col} échouée : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer_vers_float(colonne):\n",
    "    \"\"\"Nettoie une série de type texte pour la convertir en float\"\"\"\n",
    "    return (\n",
    "        colonne.astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)  # virgules → points\n",
    "        .str.replace(\n",
    "            r\"[^\\d\\.\\-]\", \"\", regex=True\n",
    "        )  # suppr. caractères sauf chiffres, . et -\n",
    "        .replace(\"\", pd.NA)  # valeurs vides → NaN\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "\n",
    "# Liste des variables à convertir\n",
    "obj_mal_encodees = df_types_sorted.query(\n",
    "    \"dtype == 'object' and n_modalités > 50\"\n",
    ")[\"nom\"]\n",
    "\n",
    "# Application\n",
    "for col in obj_mal_encodees:\n",
    "    try:\n",
    "        X_train_clean[col] = nettoyer_vers_float(X_train_clean[col])\n",
    "        print(f\"{col} convertie avec succès en float.\")\n",
    "    except Exception as e:\n",
    "        print(f\"{col} échouée : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f834d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les types\n",
    "print(X_train_clean[obj_mal_encodees].dtypes)\n",
    "\n",
    "# Ou s'assurer qu’il ne reste plus de types object inattendus\n",
    "print(X_train_clean.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8c264",
   "metadata": {
    "id": "22d8c264"
   },
   "source": [
    "L’analyse des types met en lumière plusieurs constats importants :\n",
    "\n",
    "1. De nombreuses colonnes sont typées en `object` alors qu'elles pourraient représenter des `category` (ex. : `ACTIVIT2`, `VOCATION`, `ADOSS`, etc.). Ces variables devront possiblement être converties en `category` pour une meilleure gestion mémoire et un encodage plus efficace. Cette conversion s'appuie sur le nombre de modalités uniques par colonne, permettant de distinguer les variables catégorielles des variables textuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e225f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détection des colonnes 'object' candidates à la conversion en 'category'\n",
    "object_cols = X_train_clean.select_dtypes(include=\"object\").columns\n",
    "low_card_cols = [\n",
    "    col for col in object_cols if X_train_clean[col].nunique() < 100\n",
    "]\n",
    "\n",
    "# Conversion\n",
    "for col in low_card_cols:\n",
    "    X_train_clean[col] = X_train_clean[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8301c",
   "metadata": {},
   "source": [
    "2. Quelques colonnes numériques mal typées sont également observées. Par exemple, certaines variables quantitatives (`KAPITALK` $\\forall \\; \\text{K} \\in [34;43]_{\\mathbb{N}}$, `SURFACE4` et `SURFACE6`, `TAILLE1` et `TAILLE2`, etc.) sont encodées en `object`, probablement à cause de caractères non numériques ou de formats hétérogènes. Une vérification du contenu et un nettoyage sont requis avant une conversion en `float` ou `int`.\n",
    "\n",
    "3. On note une forte hétérogénéité entre les types (`int64`, `float64`, `object`) pour des variables a priori similaires (ex. : `KAPITAL` ou `SURFACE` de 1 à 40). Une harmonisation des types est souhaitable pour garantir la cohérence du traitement.\n",
    "- La variable `ANNEE_ASSURANCE` est déjà bien typée en `float64`, ce qui est cohérent avec son interprétation comme une durée exprimée en fraction d’année.\n",
    "\n",
    "4. La présence de variables catégorielles (ex. `ACTIVIT2`, `VOCATION`, `ADOSS`) en `object` suggère qu'un encodage sera nécessaire pour les modèles de machine learning, qui ne peuvent pas traiter directement les chaînes de caractères."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iWIENFh_4G70",
   "metadata": {
    "id": "iWIENFh_4G70"
   },
   "source": [
    "## Analyse du nombre de modalités uniques par variable\n",
    "\n",
    "Cette section propose un décompte du **nombre de valeurs uniques** pour chaque variable du jeu `X_train`, trié en ordre décroissant. L’interprétation de ces résultats permettra de cibler :\n",
    "\n",
    "- Les variables constantes à supprimer ;\n",
    "- Les variables avec un nombre excessif de modalités à encoder spécifiquement ;\n",
    "- Les cas intermédiaires pouvant orienter vers un regroupement ou une transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XxhzPi5TUM3B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XxhzPi5TUM3B",
    "outputId": "d8083a35-6a83-43af-c159-b96566a6b5b1"
   },
   "outputs": [],
   "source": [
    "# Nombre de modalités uniques par variable\n",
    "unique_values = X_train.nunique()\n",
    "\n",
    "# Types des variables\n",
    "types = X_train.dtypes\n",
    "\n",
    "# Construction du DataFrame avec nom, type et nombre de modalités\n",
    "modalities_info = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"Variables\": unique_values.index,\n",
    "            \"Nombre de modalités\": unique_values.values,\n",
    "            \"Type\": types[unique_values.index].values,\n",
    "        }\n",
    "    )\n",
    "    .sort_values(by=\"Nombre de modalités\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display_scrollable_df(modalities_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc8330",
   "metadata": {},
   "source": [
    "L’analyse du nombre de valeurs distinctes par variable permet d’anticiper les traitements nécessaires avant la phase de modélisation : encodage, regroupement, réduction de dimension ou exclusion.\n",
    "\n",
    "- **Variables à forte cardinalité** :\n",
    "  - `ID` (383 610 modalités) est un identifiant unique à exclure.\n",
    "  - `ANNEE_ASSURANCE` (>1 000 modalités) présente une variabilité très élevée ; un regroupement ou une transformation sera nécessaire si la variable est conservée.\n",
    "\n",
    "- **Variables à cardinalité moyenne (10–100 modalités)** :\n",
    "  - De nombreuses variables comme `KAPITAL*`, `SURFACE*`, `RISK*`, etc., appartiennent à cette catégorie. Elles nécessiteront un **encodage approprié** : binning, ordinal encoding ou encodage fréquentiel.\n",
    "\n",
    "- **Variables à faible cardinalité (≤ 5 modalités)** :\n",
    "  - Très fréquentes dans les familles `DEROG`, `EQUIPEMENT`, `RISK`, `CARACT`, etc.\n",
    "  - Celles-ci sont candidates à un encodage one-hot ou à une exclusion si elles s’avèrent peu informatives ou redondantes.\n",
    "\n",
    "- Un grand nombre de variables, typiquement de type `object`, comportent **4 modalités ou moins**, ce qui suggère qu’elles représentent des catégories qualitatives discrètes.\n",
    "\n",
    "- **Cas particuliers** :\n",
    "  - Les variables `DISTANCE_244`, `IND_Y1_Y2`, `DEROG14` et `IND_INC` n’ont qu’une seule modalité : elles sont donc **non discriminantes** et peuvent être exclues.\n",
    "  - Des variables peu variées combinées à un taux élevé de valeurs manquantes devront faire l’objet d’un examen croisé.\n",
    "\n",
    "\n",
    "> L’analyse du nombre de modalités uniques fournit les bases nécessaires pour :\n",
    ">\n",
    "> - Identifier les **variables non informatives** à exclure (ex. : modalités constantes) ;\n",
    "> - Orienter les choix d’**encodage selon le niveau de cardinalité** (e.g. : one-hot, ordinal, fréquentiel) ;\n",
    "> - Déterminer la nécessité d’un **regroupement** (binning), d’une **transformation** ou d’une **réduction de dimension** préalable.\n",
    ">  \n",
    "> Cette étape exploratoire prépare ainsi le terrain pour l’étude plus fine des **distributions**, indispensable à la **détection des valeurs atypiques**, des **déséquilibres** ou encore à l’**ajustement des transformations**.\n",
    "\n",
    "## Analyse des distributions et détection des *outliers*\n",
    "\n",
    "Après l’examen des cardinalités, cette étape vise à explorer la répartition des valeurs dans chaque variable pour :\n",
    "\n",
    "- Identifier d’éventuelles valeurs aberrantes ou extrêmes ;\n",
    "- Détecter des déséquilibres de classes ou de modalités ;\n",
    "- Évaluer la normalité, la symétrie ou l’asymétrie des distributions ;\n",
    "- Guider les choix de transformation (e.g. log, box-cox) ou de normalisation (e.g. min-max, z-score) ;\n",
    "- Mieux comprendre le comportement des variables numériques et catégorielles.\n",
    "\n",
    "> Cette exploration s'appuira sur des visualisations graphiques (histogrammes, *boxplots*, barplots), mais aussi sur des indicateurs statistiques (min, max, médiane, quantiles), essentiels à la préparation des données pour les modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53923d39",
   "metadata": {},
   "source": [
    "### Lister les variables par tranches de cardinalité\n",
    "\n",
    "Pour faciliter l’analyse des distributions, les variables de `X_train` sont regroupées par tranches de cardinalité. Cette classification permet de cibler les traitements appropriés en fonction du nombre de modalités :\n",
    "- **Variables à faible cardinalité** (1 à 5 modalités) : souvent catégorielles, elles peuvent être encodées en one-hot ou ordinal encoding.\n",
    "- **Variables à cardinalité moyenne** (6 à 100 modalités) : nécessitent un encodage fréquentiel ou un regroupement.\n",
    "- **Variables à forte cardinalité** (plus de 100 modalités) : souvent numériques, elles peuvent nécessiter une transformation ou une normalisation.\n",
    "- **Variables constantes** (1 modalité) : à exclure car non informatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e064bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colonnes catégorielles et numériques\n",
    "categorical_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(\n",
    "    include=[\"float64\", \"int64\"]\n",
    ").columns.tolist()\n",
    "n_modalites = [X_train[col].nunique() for col in categorical_cols]\n",
    "\n",
    "modalities = pd.Series(n_modalites, index=categorical_cols).sort_values(\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "bins = {\n",
    "    \"≤ 2 modalités\": modalities[modalities <= 2],\n",
    "    \"3-5 modalités\": modalities[(modalities > 2) & (modalities <= 5)],\n",
    "    \"6-10 modalités\": modalities[(modalities > 5) & (modalities <= 10)],\n",
    "    \"> 10 modalités\": modalities[modalities > 10],\n",
    "}\n",
    "\n",
    "for label, group in bins.items():\n",
    "    print(\n",
    "        f\"\\n{label} ({len(group)} variables) :\\n\", group.head(10).to_string()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03145ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des variables par type\n",
    "categorical_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(\n",
    "    include=[\"float64\", \"int64\"]\n",
    ").columns.tolist()\n",
    "\n",
    "n_modalites = [X_train[col].nunique() for col in categorical_cols]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(n_modalites, bins=30, edgecolor=\"black\")\n",
    "plt.title(\n",
    "    \"Distribution du nombre de modalités uniques des variables catégorielles\"\n",
    ")\n",
    "plt.xlabel(\"Nombre de modalités\")\n",
    "plt.ylabel(\"Nombre de variables\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd48604",
   "metadata": {},
   "source": [
    "L’histogramme de la cardinalité des variables catégorielles ci-dessus offre une perspective globale sur leur distribution, et permet d’identifier :\n",
    "\n",
    "- la proportion de variables peu variées ($\\leq$ 5 modalités), généralement simples à encoder ;\n",
    "- l’existence éventuelle de variables à cardinalité élevée, nécessitant des traitements spécifiques.\n",
    "\n",
    "Cependant, cette représentation ne fournit **aucune information sur l’identité des variables concernées**, ce qui **limite son usage opérationnel**. Il est donc **impossible de cibler des variables précises** à partir de ce graphique.\n",
    "\n",
    "En conséquence, l’histogramme peut être conservé à titre **illustratif**, mais ne remplace pas une synthèse structurée par variable, qui seule permet de prendre des décisions de prétraitement (encodage, regroupement, exclusion).\n",
    "\n",
    "Affichons donc le résumé synthétique des variables catégorielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14564aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé synthétique des variables catégorielles\n",
    "summary = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    vc = X_train[col].value_counts(dropna=False)\n",
    "    top_value = vc.index[0]\n",
    "    top_freq = vc.iloc[0]\n",
    "    top_prop = round(top_freq / len(X_train) * 100, 2)\n",
    "    summary.append(\n",
    "        {\n",
    "            \"Variable\": col,\n",
    "            \"Modalités uniques\": X_train[col].nunique(dropna=False),\n",
    "            \"Modalité la plus fréquente\": top_value,\n",
    "            \"Fréquence (%)\": top_prop,\n",
    "        }\n",
    "    )\n",
    "\n",
    "cat_summary_df = pd.DataFrame(summary).sort_values(\n",
    "    by=\"Fréquence (%)\", ascending=False\n",
    ")\n",
    "# Affichage dans une boîte scrollable\n",
    "html_summary = cat_summary_df.to_html(index=False)\n",
    "HTML(\n",
    "    f\"\"\"\n",
    "    <div style=\"height:300px; overflow-y:scroll;\n",
    "                border:1px solid lightgray; padding:10px\">\n",
    "        {html_summary}\n",
    "    </div>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcad589",
   "metadata": {},
   "source": [
    "Effectuons une démarche analogue à celle réalisée précédemment pour les variables numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58672587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé synthétique des variables numériques\n",
    "summary = []\n",
    "\n",
    "for col in numerical_cols:\n",
    "    vc = X_train[col].value_counts(dropna=False)\n",
    "    top_value = vc.index[0]\n",
    "    top_freq = vc.iloc[0]\n",
    "    top_prop = round(top_freq / len(X_train) * 100, 2)\n",
    "    summary.append(\n",
    "        {\n",
    "            \"Variable\": col,\n",
    "            \"Modalités uniques\": X_train[col].nunique(dropna=False),\n",
    "            \"Modalité la plus fréquente\": top_value,\n",
    "            \"Fréquence (%)\": top_prop,\n",
    "        }\n",
    "    )\n",
    "\n",
    "cat_summary_df = pd.DataFrame(summary).sort_values(\n",
    "    by=\"Fréquence (%)\", ascending=False\n",
    ")\n",
    "# Affichage dans une boîte scrollable\n",
    "html_summary = cat_summary_df.to_html(index=False)\n",
    "HTML(\n",
    "    f\"\"\"\n",
    "    <div style=\"height:300px; overflow-y:scroll;\n",
    "                border:1px solid lightgray; padding:10px\">\n",
    "        {html_summary}\n",
    "    </div>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C5HB8JMPYPnL",
   "metadata": {
    "id": "C5HB8JMPYPnL"
   },
   "source": [
    "### Analyse quantitative des variables catégorielles\n",
    "\n",
    "#### 1. Nombre de modalités\n",
    "- Il apparaît une **grande hétérogénéité** dans le nombre de modalités :\n",
    "  - Certaines variables présentent une très forte cardinalité, comme `ANNEE_ASSURANCE` avec 1113 modalités distinctes. Cela peut poser des problèmes en modélisation, notamment pour les algorithmes sensibles à la granularité des données ;\n",
    "  - À l'inverse, un grand nombre de variables n'ont que **2 à 4 modalités**, comme `ADOSS`, `TYPERS`, `DEROG1`, etc.\n",
    "- Plus de **150 variables** possèdent **moins de 5 modalités**, ce qui les rend **faciles à encoder** (binaire, label encoding...).\n",
    "- Une large famille de variables telles que les `DISTANCE_xxx`, `RR_xxx`, `NBJ_xxx`, `FXI_xxx`, `TMM/TX/TN_xxx` ont souvent **4 classes ordinales** correspondant à des intervalles de valeurs (souvent par quartiles).\n",
    "\n",
    "#### 2. Distribution déséquilibrée\n",
    "- Beaucoup de variables présentent une **forte domination d'une modalité** :\n",
    "  - Exemple : `DEROG1` → 99.98% de \"N\", 0.02% de \"O\".\n",
    "  - `KAPITAL38` → 99.96% de \"N\".\n",
    "- D'autres variables comme `ACTIVIT2`, `VOCATION`, `CARACT4`, `MEN_1IND` sont déséquilibrées mais exploitables.\n",
    "\n",
    "#### 3. Présence de modalités rares\n",
    "- Certaines modalités apparaissent **moins de 10 fois** :\n",
    "  - Cela peut poser des problèmes de **sur-apprentissage** et **brouiller le signal** dans les modèles.\n",
    "  - Exemples : `DEROG14`, `DISTANCE_244`, certaines modalités de `MEN_PROP`.\n",
    "\n",
    "#### 4. Codages textuels ordonnés\n",
    "- De nombreuses variables sont issues de **discrétisation d'une variable continue** :\n",
    "  - Exemples de modalités : `01. <= 10`, `04. >= 40`, etc.\n",
    "  - Ces variables conservent une **structure ordinale** exploitable :\n",
    "    - **Encodage ordinal**\n",
    "    - Ou **reconversion en continu** via la borne médiane\n",
    "\n",
    "#### 5. Familles de variables cohérentes\n",
    "- Plusieurs familles homogènes peuvent être identifiées :\n",
    "  - `DISTANCE_xxx` : distances en classes (4-5 modalités).\n",
    "  - `NBJxxx_MM/MSOM/MMAX` : séries météo discrétisées.\n",
    "  - `PROPORTION_xx`, `MEN_xxx`, `KAPITALxx`, `DEROGxx` : codifications internes.\n",
    "- Ces regroupements permettent une **ingénierie de variables pertinente** :\n",
    "  - Moyennes ou scores internes\n",
    "  - Réduction de dimension (ACP)\n",
    "  - Agrégation par famille\n",
    "\n",
    "### Implications pour la modélisation\n",
    "\n",
    "- **Variables peu informatives** ou trop déséquilibrées :\n",
    "  - À écarter ou traiter spécifiquement.\n",
    "  - Ex : `DEROG14`, `IND_INC`, `DISTANCE_244` ;\n",
    "\n",
    "- **Variables ordinales exploitables** :\n",
    "  - Les variables comme `DISTANCE`, `METEO`, `PROPORTION`, etc., sont **structurées pour un encodage ordonné ou continu** ;\n",
    "\n",
    "- **Variables très dispersées** :\n",
    "  - Ex : `ANNEE_ASSURANCE`, `ZONE`\n",
    "  - Nécessitent un traitement spécifique : **groupement**, **encodage fréquence**, ou **embedding** selon le modèle.\n",
    "\n",
    "L'approche implémentée ici repose sur une stratégie **différenciée selon la nature et la complétude des variables**, avec les objectifs suivants :\n",
    "\n",
    "- **Préserver l'information existante** dans les colonnes peu affectées par des valeurs manquantes ;\n",
    "- **Limiter la distorsion** causée par l'imputation dans les colonnes fortement incomplètes ;\n",
    "- **Permettre au modèle d'apprendre** à partir du fait qu'une valeur était absente, en créant une modalité explicite (`'missing'`) pour les variables catégorielles très incomplètes ;\n",
    "\n",
    "#### Logique appliquée :\n",
    "- **Variables numériques :**\n",
    "  - Si moins de 30% de valeurs manquantes → imputation par la **médiane** (robuste aux outliers) ;\n",
    "  - Si plus de 30% de valeurs manquantes → imputation conservatrice par **zéro**, considérée comme une absence de valeur pertinente.\n",
    "\n",
    "- **Variables catégorielles :**\n",
    "  - Si moins de 30% de valeurs manquantes → imputation par la **valeur la plus fréquente** (mode);\n",
    "  - Si plus de 30% de valeurs manquantes → imputation par une valeur explicite `'missing'`, interprétable comme une modalité à part entière.\n",
    "\n",
    "Cette stratégie permet une **modélisation plus robuste**, tout en **évitant de supprimer prématurément des variables potentiellement informatives** malgré leur incomplétude.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2LtI39Qk8Oy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2LtI39Qk8Oy",
    "outputId": "1d03f603-8e35-40f6-c84d-e7a9e740c2e3"
   },
   "outputs": [],
   "source": [
    "print(\"ID\" in X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qYrrrKz1bTW6",
   "metadata": {
    "id": "qYrrrKz1bTW6"
   },
   "outputs": [],
   "source": [
    "# Séparation colonnes numériques / catégorielles\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# 1. Colonnes numériques\n",
    "# a. Colonnes avec peu de valeurs manquantes (<30%) → imputation par la médiane\n",
    "low_na_num = [col for col in num_cols if X_train[col].isna().mean() < 0.3]\n",
    "# b. Colonnes très incomplètes (>=30%) → imputation conservatrice par constante\n",
    "high_na_num = [col for col in num_cols if X_train[col].isna().mean() >= 0.3]\n",
    "\n",
    "X_train[low_na_num] = SimpleImputer(strategy=\"median\").fit_transform(\n",
    "    X_train[low_na_num]\n",
    ")\n",
    "X_train[high_na_num] = X_train[high_na_num].fillna(0)\n",
    "\n",
    "# 2. Colonnes catégorielles\n",
    "# a. Colonnes avec peu de NaN → imputation par la valeur la plus fréquente\n",
    "low_na_cat = [col for col in cat_cols if X_train[col].isna().mean() < 0.3]\n",
    "# b. Colonnes très incomplètes → imputation par 'missing' (valeur explicite)\n",
    "high_na_cat = [col for col in cat_cols if X_train[col].isna().mean() >= 0.3]\n",
    "\n",
    "X_train[low_na_cat] = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "    X_train[low_na_cat]\n",
    ")\n",
    "X_train[high_na_cat] = X_train[high_na_cat].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jOG8CqnScON3",
   "metadata": {
    "id": "jOG8CqnScON3"
   },
   "source": [
    "Avant d'entraîner un modèle, il est essentiel d'écarter certaines variables qui **n'apportent pas d'information pertinente pour la prédiction**, ou qui pourraient **introduire du bruit ou des fuites de données** :\n",
    "\n",
    "- **`ID`** :  \n",
    "  - Identifiant unique de la ligne ou de l'observation ;\n",
    "  - **Sans signification statistique** : il n'est pas corrélé à la cible et risque d'être utilisé à tort par un modèle non régularisé comme un pseudo-prédicteur.\n",
    "  - Peut **induire du surapprentissage** si laissé dans les données.\n",
    "\n",
    "- **`ANNEE_ASSURANCE`** :  \n",
    "  - Variable qui peut être fortement **corrélée avec la date de déclaration**, de souscription ou avec le risque (biais temporel).\n",
    "  - Si la modélisation est censée être **générique et prédictive hors période**, cette colonne introduit un **risque de fuite temporelle**.\n",
    "  - À moins de construire un modèle par période ou de vouloir capter des effets saisonniers ou historiques, il est préférable de la supprimer.\n",
    "\n",
    "> Ces suppressions participent à la **robustesse** du modèle en limitant l'apprentissage sur des artefacts sans valeur prédictive réelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vJ1qbZm7ceRs",
   "metadata": {
    "id": "vJ1qbZm7ceRs"
   },
   "outputs": [],
   "source": [
    "# Suppression des colonnes inutiles\n",
    "X_train = X_train.drop([\"ID\", \"ANNEE_ASSURANCE\"], axis=1)\n",
    "\n",
    "# Encodage des variables catégoriques avec CountEncoder\n",
    "encoder = CountEncoder(cols=cat_cols)\n",
    "encoder.fit(X_train)\n",
    "X_train_enc = encoder.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voaAuyJRmpUy",
   "metadata": {
    "id": "voaAuyJRmpUy"
   },
   "source": [
    "### Graphe de distribution de la variable cible `CM` (coût moyen)\n",
    "\n",
    "Le graphe de distribution de la variable cible s'inscrit dans trois objectifs :\n",
    "- Détection de la distribution (loi normale, asymétrique, etc.) : Si la distribution est fortement asymétrique , cela peut indiquer qu'une transformation sera nécessaire pour améliorer les performances du modèle ;\n",
    "- Identification d'éventuelles valeurs extrêmes (*outliers*) : Les éléments isolés à l'extrémité du graphique révèlent des valeurs atypiques, qui peuvent biaiser l'apprentissage si non traitées ;\n",
    "- Ajustement du modèle et du choix de la métrique : Une distribution très déséquilibrée ou avec des zéros majoritaires peut influencer le choix de la fonction de perte ou des stratégies de rééchantillonnage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6s0Pm4FNl89P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "6s0Pm4FNl89P",
    "outputId": "5ef22ab0-2ec6-470a-e947-5f6f28b07f08"
   },
   "outputs": [],
   "source": [
    "# Graphe de distribution de y_train['CM']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_train[\"CM\"], kde=True)\n",
    "plt.title(\"Distribution of CM\")\n",
    "plt.xlabel(\"CM\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YIk0tgZGn-tL",
   "metadata": {
    "id": "YIk0tgZGn-tL"
   },
   "source": [
    "La distribution du coût moyen (CM) présente les caractéristiques suivantes :\n",
    "\n",
    "- Distribution extrêmement asymétrique à droite :\n",
    "  - Une immense majorité des valeurs de CM sont proches de zéro ;\n",
    "  - Quelques valeurs atteignent des niveaux très élevés (jusqu'à près de 500000), mais elles sont très rares.\n",
    "\n",
    "- Concentration des observations :\n",
    "  - Plus de 3 millions d'observations se situent dans l'intervalle 0 à quelques milliers d'euros, ce qui indique une forte concentration autour de petites valeurs ;\n",
    "  - Cela suggère que la majorité des sinistres ont un coût moyen faible.\n",
    "\n",
    "- Présence de valeurs extrêmes (outliers) :\n",
    "  - Quelques sinistres présentent un coût exceptionnellement élevé.\n",
    "  - Ces valeurs ont un poids potentiel important dans le calcul des métriques(RMSE, MAE…) et peuvent biaisser l'entraînement si non traitées.\n",
    "\n",
    "L'examen de la distribution brute de `CM` révèle une **forte asymétrie à droite** : la majorité des valeurs sont très faibles (proches de zéro), tandis qu'une minorité présente des montants exceptionnellement élevés.\n",
    "\n",
    "Ce comportement est typique des variables de type **coût de sinistres** ou **montants financiers**, souvent modélisées avec des **lois de Pareto**, **log-normales** ou **gamma**.\n",
    "\n",
    "Une transformation logarithmique `log(CM + 1)` permettrait de :\n",
    "  - Réduire l'effet des *outliers* ;\n",
    "  - Rapprocher la distribution d'une loi normale, facilitant la modélisation ;\n",
    "  - Améliorer la performance des modèles sensibles à la distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MzXOmI-LpLdO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "MzXOmI-LpLdO",
    "outputId": "4f2d42c5-1be8-4b61-a7a7-ac20c1da09e6"
   },
   "outputs": [],
   "source": [
    "# Transformation log(CM + 1) pour réduire la variance et la dissymétrie\n",
    "y_train[\"CM_log\"] = np.log1p(y_train[\"CM\"])  # log1p(x) = log(x + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_train[\"CM_log\"], kde=True, bins=50)\n",
    "plt.title(\"Distribution de log(CM + 1)\")\n",
    "plt.xlabel(\"log(CM + 1)\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qeB2lnK3p0rK",
   "metadata": {
    "id": "qeB2lnK3p0rK"
   },
   "source": [
    "Même après transformation `log(CM + 1)`, la distribution reste ultra-concentrée autour de zéro, ce qui n'est pas l'effet attendu d'un `log-transform`. Cela suggère fortement que **la majorité des valeurs de CM sont nulles ou quasi-nulles**.\n",
    "Vérifions cette hypothèse en deux temps :\n",
    "1. Vérifier la proportion de zéros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rvrWsBl2qdU4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvrWsBl2qdU4",
    "outputId": "0c234528-de22-44cf-99e9-9dae565f2430"
   },
   "outputs": [],
   "source": [
    "zero_ratio = (y_train[\"CM\"] == 0).mean()\n",
    "print(f\"Proportion de zéros dans CM : {zero_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aEPMIgpyqkE8",
   "metadata": {
    "id": "aEPMIgpyqkE8"
   },
   "source": [
    "2. Affiche la distribution sans les zéros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gFmBtJ8rqkm6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "gFmBtJ8rqkm6",
    "outputId": "7036f1e3-298c-4c3e-ef17-1f8e47c2c5f9"
   },
   "outputs": [],
   "source": [
    "non_zero_cm = y_train[y_train[\"CM\"] > 0][\"CM\"]\n",
    "log_non_zero_cm = np.log1p(non_zero_cm)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(log_non_zero_cm, kde=True, bins=50)\n",
    "plt.title(\"Distribution de log(CM + 1) pour CM > 0\")\n",
    "plt.xlabel(\"log(CM + 1)\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6TaQKK2NrDVe",
   "metadata": {
    "id": "6TaQKK2NrDVe"
   },
   "source": [
    "Plusieur éléments se dégagent de la transformation logarithmique sur les observations strictement positives (`CM > 0`) :\n",
    "\n",
    "1. **Réduction de l'asymétrie**  \n",
    "   La transformation logarithmique a permis de corriger l'asymétrie extrême initialement observée dans la distribution brute de `CM`. Elle :\n",
    "   - compresse l'effet des très grandes valeurs (*outliers*),\n",
    "   - met mieux en évidence la structure sous-jacente des montants courants.\n",
    "\n",
    "2. **Distribution toujours asymétrique à droite**  \n",
    "   Malgré la transformation, la distribution reste légèrement étalée vers la droite, ce qui reflète la nature typiquement déséquilibrée des montants de sinistres (une majorité de petits montants, quelques très gros).\n",
    "\n",
    "3. Le mode, qui constitue la valeur la plus fréquente dans une série de données, se situe autour de `log(CM + 1) ≈ 6`: Cela correspond à un coût de sinistre typique de `exp(6) - 1 ≈ 400 €`, valeur qui semble représenter un sinistre \"standard\" dans le jeu de données.\n",
    "\n",
    "Nous pouvons en déduire que cette transformation rend la variable cible plus adaptée à une régression, notamment linéaire, en :\n",
    "  - réduisant la variance,\n",
    "  - limitant le poids des valeurs extrêmes,\n",
    "  - facilitant la convergence des modèles.\n",
    "\n",
    "- Elle améliore également la **robustesse** des algorithmes face aux déséquilibres structurels inhérents aux données assurantielles.\n",
    "\n",
    "> Cette transformation est particulièrement utile dans une stratégie en deux étapes :  \n",
    "> 1. modèle binaire de prédiction de la survenue d'un sinistre (`CM > 0`),  \n",
    "> 2. régression sur `log(CM + 1)` conditionnée à `CM > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aOvLW_N7r0Rs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOvLW_N7r0Rs",
    "outputId": "a7c55d42-b52f-4aa2-d519-276b8508ee6e"
   },
   "outputs": [],
   "source": [
    "# 1. Création de la cible binaire (sinistre ou non)\n",
    "y_train_bin = (y_train[\"CM\"] > 0).astype(int)\n",
    "\n",
    "# Séparation des données pour la phase de test\n",
    "X_tr, X_val, y_bin_tr, y_bin_val = train_test_split(\n",
    "    X_train_enc, y_train_bin, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Étape 1 - Modèle de classification binaire : prédire si CM > 0\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_tr, y_bin_tr)\n",
    "\n",
    "# Prédictions de probabilité d’avoir un sinistre\n",
    "proba_sinistre = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Évaluation du modèle binaire\n",
    "auc = roc_auc_score(y_bin_val, proba_sinistre)\n",
    "print(f\"AUC (CM > 0): {auc:.4f}\")\n",
    "\n",
    "# 3. Étape 2 - Modèle de régression conditionnelle sur CM > 0\n",
    "# Filtrage sur les cas avec sinistre\n",
    "X_train_pos = X_train_enc[y_train[\"CM\"] > 0]\n",
    "y_train_logcm = np.log1p(y_train[\"CM\"][y_train[\"CM\"] > 0])\n",
    "\n",
    "# Séparation pour validation\n",
    "X_tr2, X_val2, y_tr2, y_val2 = train_test_split(\n",
    "    X_train_pos, y_train_logcm, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Régression sur log(CM + 1)\n",
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X_tr2, y_tr2)\n",
    "\n",
    "# Prédictions\n",
    "y_pred_log = reg.predict(X_val2)\n",
    "rmse = mean_squared_error(y_val2, y_pred_log)\n",
    "print(f\"RMSE sur log(CM + 1): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U_FRvBllw3Z0",
   "metadata": {
    "id": "U_FRvBllw3Z0"
   },
   "source": [
    "#### Analyse des performances du modèle\n",
    "\n",
    "#### 1. AUC (CM > 0) = **0.7371**\n",
    "\n",
    "L'AUC (*Area Under the Curve*) évalue la capacité du modèle à distinguer les observations où `CM > 0` (existence d'un sinistre) de celles où `CM = 0` (absence de sinistre).  \n",
    "Une AUC de **0.7371** indique une **bonne performance de discrimination** :  \n",
    "- Le modèle parvient à capter un signal pertinent.\n",
    "- Il est significativement meilleur que le hasard (AUC = 0.5).\n",
    "- Cela ouvre la voie à des **applications utiles pour le scoring ou la priorisation du risque**.\n",
    "\n",
    "#### 2. RMSE sur `log(CM + 1)` = **5.5240**\n",
    "\n",
    "Le RMSE est calculé ici sur la transformation logarithmique `log(CM + 1)`, souvent utilisée pour réduire l'impact des outliers dans une distribution très asymétrique.  \n",
    "Une erreur de 5.5240 en log se traduit par une erreur moyenne d’environ :\n",
    "\n",
    "$$\n",
    "e^{5.5240} - 1 \\approx 250 \\text{ €}\n",
    "$$\n",
    "\n",
    "Cela signifie qu'en moyenne, le modèle se trompe d'environ 250 € sur la prédiction du coût sinistre.  \n",
    "C'est une erreur significative si la plupart des sinistres sont faibles, mais raisonnable si la distribution comporte de nombreux sinistres très élevés.\n",
    "\n",
    "Ainsi :\n",
    "\n",
    "- Le modèle **discrimine correctement** la survenue d'un sinistre (AUC satisfaisante).\n",
    "- L'estimation du coût (RMSE) est perfectible, probablement à cause de la **présence d’outliers** importants.\n",
    "- Des améliorations possibles :\n",
    "  - Approche **en deux étapes** : classification (CM > 0) + régression sur les cas sinistrés.\n",
    "  - Utilisation de modèles **robustes aux valeurs extrêmes**.\n",
    "  - Test de **transformations alternatives** ou pondérations adaptées au domaine assurantiel.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "assurprime_modeling_workflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
