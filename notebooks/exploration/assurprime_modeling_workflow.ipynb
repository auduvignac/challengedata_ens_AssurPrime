{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3e36429d",
      "metadata": {
        "id": "3e36429d"
      },
      "source": [
        "# AssurPrime : Saurez-vous prédire la prime d'assurance ?\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/auduvignac/challengedata_ens_AssurPrime/blob/main/notebooks/exploration/assurprim_modeling_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f90a2dfa",
      "metadata": {},
      "source": [
        "# Contexte\n",
        "\n",
        "Crédit Agricole Assurances est une filiale du Groupe Crédit Agricole dédiée à l’assurance, faisant de celui-ci un acteur multi-expert de la bancassurance et le 1er bancassureur en Europe.\n",
        "Crédit Agricole Assurances regroupe plusieurs entités, dont Predica et Pacifica, qui proposent une large gamme d’assurances aux particuliers, aux exploitants agricoles, aux professionnels et aux entreprises. Crédit Agricole Assurances s’engage à offrir des solutions innovantes et adaptées aux besoins des clients, tout en favorisant le développement durable et la responsabilité sociale.\n",
        "Au sein de l’Académie Data Science du groupe, l’objectif est de participer activement à la montée en compétences des collaborateurs, de partager des connaissances et d’identifier de nouveaux usages.\n",
        "\n",
        "# Objectif\n",
        "\n",
        "Le contrat Multirisque Agricole, géré par Pacifica, est souscrit par les agriculteurs pour sécuriser leur exploitation. Il couvre l’activité professionnelle, les dommages aux bâtiments d’exploitation, le matériel stocké, ainsi que la protection financière et juridique. Ce contrat garantit à l’assuré une couverture efficace et durable, assurant ainsi la continuité de son activité en cas de sinistre, tant sur le plan matériel que financier.\n",
        "\n",
        "Actuellement, le risque d’incendie constitue une part majeure de la charge sinistre du contrat Multirisque Agricole, ce qui en fait un enjeu clé à modéliser avec précision.\n",
        "\n",
        "L’objectif est d’identifier le meilleur modèle pour prédire la prime pure incendie, en utilisant :\n",
        "- Un modèle pour la Fréquence,\n",
        "- Un modèle pour le Coût moyen.\n",
        "\n",
        "La variable cible finale, la charge, est obtenue en multipliant la fréquence, le coût moyen, et le nombre d’années depuis la souscription du contrat (la variable `ANNEE_ASSURANCE`).\n",
        "\n",
        "# Description des données\n",
        "\n",
        "Un fichier supplémentaire est mis à disposition regroupant toutes les variables disponibles, accompagnées de leur description. Ce fichier inclut :\n",
        "\n",
        "- **Les variables cibles** : `FREQ`, `CM`, et `CHARGE`\n",
        "- **Données géographiques** : département, données météorologiques, etc.\n",
        "- **Données spécifiques au contrat**, notamment :\n",
        "    - L’activité de l’assuré (cultivateur, polyculteur, etc.)\n",
        "    - Les indicateurs de souscription des garanties\n",
        "    - Le nombre de bâtiments, de salariés, et de sinistres déclarés lors de la souscription\n",
        "    - **Données de surface** : surfaces des bâtiments (élevage, exploitation, etc.), anonymisées en `surface1`, `surface2`, etc., pour garantir la confidentialité\n",
        "    - **Données de capitaux** : capitaux assurés pour différentes options (vol, serres, etc.), anonymisés en `capital1`, `capital2`, etc.\n",
        "    - **Données liées à la prévention** : présence d’équipements (extincteurs, structure en bois, etc.), anonymisées en `prev1`, `prev2`, etc.\n",
        "\n",
        "\n",
        "# Description du *benchmark*\n",
        "\n",
        "## Objectif du challenge\n",
        "\n",
        "L’objectif de ce challenge est de comparer les performances des modèles développés dans le cadre de cette compétition avec celles d’un modèle de référence basé sur des **GLM (Generalized Linear Models)** classiques.\n",
        "\n",
        "## Structure du benchmark\n",
        "\n",
        "Le benchmark repose sur deux modèles **GLM** distincts :\n",
        "\n",
        "- Fréquence des sinistres :\n",
        "    - Distribution : *Loi de Poisson* ;\n",
        "    - Fonction de lien : *Log*.\n",
        "\n",
        "- Coût moyen d’un sinistre :\n",
        "    - Distribution : *Tweedie* ;\n",
        "    - Fonction de lien : *Log*.\n",
        "\n",
        "## Évaluation\n",
        "\n",
        "L’évaluation des modèles repose sur une métrique unique : **RMSE (Root Mean Square Error)**, définie par la formule suivante :\n",
        "\n",
        "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
        "\n",
        "où :\n",
        "- $y_i$ représente la valeur réelle ;\n",
        "- $\\hat{y}_i$ représente la valeur prédite ;\n",
        "- $n$ est le nombre d’observations.\n",
        "\n",
        "L’idée reste d’évaluer dans quelle mesure les approches proposées permettent de dépasser les performances des modèles standards tout en prenant en compte :\n",
        "\n",
        "- La précision des prédictions ;\n",
        "- Les aspects d’interprétabilité et d’efficacité ;\n",
        "- Les contraintes métier associées."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa7c3cc",
      "metadata": {
        "id": "4aa7c3cc"
      },
      "source": [
        "## Bibliothèques utilisées\n",
        "\n",
        "Installation et importation des bibliothèques nécessaires à l’analyse, couvrant l’ensemble du processus de traitement des données, de modélisation et d’évaluation :\n",
        "\n",
        "- `pandas`, `numpy` : manipulation de données numériques et tabulaires ;\n",
        "- `matplotlib.pyplot`, `seaborn` : visualisation pour l’exploration et l’analyse des résultats ;\n",
        "- `scipy.stats.chi2_contingency` : test statistique sur les variables qualitatives ;\n",
        "- `scikit-learn` : imputation (`SimpleImputer`), standardisation (`StandardScaler`), modélisation (régression logistique, ridge, gradient boosting), validation croisée et évaluation des performances (RMSE, F-score, AUC, etc.) ;\n",
        "- `xgboost` : modélisation via des algorithmes de boosting (`XGBClassifier`, `XGBRegressor`) ;\n",
        "- `category_encoders.CountEncoder` : encodage des variables catégorielles par fréquence d’apparition ;\n",
        "- `IPython.display.display` : affichage contrôlé des objets dans le notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61211875",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61211875",
        "outputId": "4589ac86-5bfa-4135-ab13-ef1efac4e734"
      },
      "outputs": [],
      "source": [
        "!pip install -q category-encoders \\\n",
        "                matplotlib \\\n",
        "                numpy \\\n",
        "                pandas \\\n",
        "                scikit-learn \\\n",
        "                seaborn \\\n",
        "                xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d64730ff",
      "metadata": {
        "id": "d64730ff"
      },
      "outputs": [],
      "source": [
        "from category_encoders import CountEncoder\n",
        "from IPython.display import display\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.metrics import classification_report, fbeta_score, mean_squared_error, make_scorer, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8efe7adf",
      "metadata": {
        "id": "8efe7adf"
      },
      "source": [
        "## Chargement et aperçu des données\n",
        "\n",
        "Chargement des fichiers de données d'entraînement à partir des sources distantes. Les fichiers comprennent :\n",
        "\n",
        "- `X_train` : variables explicatives issues des contrats d’assurance ;\n",
        "- `y_train` : variables cibles associées (fréquence, coût moyen, charge).\n",
        "\n",
        "Un aperçu des premières lignes sera ensuite affiché afin de vérifier la structure et le contenu des jeux de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadae0aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "fadae0aa",
        "outputId": "21b85269-12b7-4361-a532-d5c89d494cb2"
      },
      "outputs": [],
      "source": [
        "# Chargement des données\n",
        "try:\n",
        "    print(\"Chargement des données...\")\n",
        "\n",
        "    X_train_url = \"https://media.githubusercontent.com/media/auduvignac/challengedata_ens_AssurPrime/refs/heads/main/data/raw/x_train.csv\"\n",
        "    X_train = pd.read_csv(X_train_url)\n",
        "\n",
        "    y_train_url = \"https://media.githubusercontent.com/media/auduvignac/challengedata_ens_AssurPrime/refs/heads/main/data/raw/y_train.csv\"\n",
        "    y_train = pd.read_csv(y_train_url)\n",
        "\n",
        "    print(\"Données chargées avec succès.\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Erreur lors du chargement des données. Vérifiez que les URLs sont correctes.\")\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d658668",
      "metadata": {},
      "source": [
        "## Aperçu des variables explicatives (`X_train`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7793b165",
      "metadata": {},
      "outputs": [],
      "source": [
        "# aperçu de X_train\n",
        "print(\"Aperçu de X_train :\")\n",
        "display(X_train.head())\n",
        "# Affichage du nombre d'observations\n",
        "display(f\"Nombre d'observations : {len(X_train)}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1b14543",
      "metadata": {},
      "source": [
        "Le jeu de données `X_train` contient les variables explicatives issues des contrats d’assurance. Chaque ligne correspond à un contrat souscrit dans le cadre du produit Multirisque Agricole.\n",
        "\n",
        "Les premières colonnes illustrent la diversité des informations disponibles :\n",
        "\n",
        "- **Variables catégorielles liées au contrat** (`ACTIVIT2`, `VOCATION`, `TYPERS`, etc.) ;\n",
        "- **Indicateurs de caractéristiques techniques ou déclaratives** (`CARACT1`, `CARACT2`, etc.) ;\n",
        "- **Variables météorologiques et historiques** (préfixes `NBJRR`, `RR_VOR`, `RRAB_VOR`, etc.) ;\n",
        "- **Variables numériques** comme `ANNEE_ASSURANCE`, représentant le nombre d’années depuis la souscription.\n",
        "\n",
        "Certaines colonnes contiennent des valeurs manquantes ou anonymisées (valeurs encodées ou plages de classes), qu’il conviendra de traiter lors de la phase de prétraitement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7207ef69",
      "metadata": {},
      "source": [
        "## Aperçu des variables cibles (`y_train`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daf6cb36",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nAperçu de y_train :\")\n",
        "display(y_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75b54628",
      "metadata": {},
      "source": [
        "Le jeu de données `y_train` contient les variables cibles associées à chaque contrat :\n",
        "\n",
        "- `FREQ` : fréquence des sinistres liés à l’incendie ;\n",
        "- `CM` : coût moyen des sinistres ;\n",
        "- `ANNEE_ASSURANCE` : nombre d’années écoulées depuis la souscription du contrat ;\n",
        "- `CHARGE` : variable cible finale.\n",
        "\n",
        "Conformément à la définition métier indiquée dans le contexte du projet, la charge est calculée de la manière suivante :\n",
        "\n",
        "$$\n",
        "\\text{CHARGE} = \\text{FREQ} \\times \\text{CM} \\times \\text{ANNEE\\_ASSURANCE}\n",
        "$$\n",
        "\n",
        "Dans l’échantillon présenté, les premières lignes montrent des charges nulles, en lien avec l'absence de sinistres (`FREQ = 0`). Ce déséquilibre devra être pris en compte lors de la modélisation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f0899b",
      "metadata": {},
      "source": [
        "## Premiers Constats\n",
        "\n",
        "- Le jeu contient **`n` observations**, alignées avec celles de `X_train` via la colonne `ID`.\n",
        "- Un **déséquilibre important** est observé : la majorité des contrats présentent une fréquence nulle (`FREQ = 0`), ce qui conduit à une charge nulle (`CHARGE = 0`).\n",
        "- **Des valeurs manquantes** sont présentes dans certaines colonnes, nécessitant un traitement adéquat lors de la phase de prétraitement.\n",
        "\n",
        "Ces éléments devront être pris en compte pour garantir la robustesse et la pertinence des modèles prédictifs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a5e6ed",
      "metadata": {
        "id": "c7a5e6ed"
      },
      "source": [
        "## Inspection des données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da6d329",
      "metadata": {
        "id": "0da6d329"
      },
      "source": [
        "### Types de colonnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdeed55b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fdeed55b",
        "outputId": "2fd1597d-3202-4269-c214-f56c1d3f28b2"
      },
      "outputs": [],
      "source": [
        "# Affichage du type des données de X_train\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "print(\"Types des données de X_train :\")\n",
        "print(X_train.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22d8c264",
      "metadata": {
        "id": "22d8c264"
      },
      "source": [
        "Le nombre de variables étant conséquent, affichons-en un résumé pour mieux appréhender leur nature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491da6cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "491da6cf",
        "outputId": "38dedbcc-90ef-4496-a7fa-4f15d76f05d6"
      },
      "outputs": [],
      "source": [
        "# Affichage synthétique des types de données\n",
        "print(\"Résumé des types de données dans X_train :\")\n",
        "print(X_train.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZvlzVgfg3anY",
      "metadata": {
        "id": "ZvlzVgfg3anY"
      },
      "source": [
        "### Analyse des valeurs manquantes\n",
        "\n",
        "Afin d'évaluer la qualité des données, analysons la quantité de valeurs manquantes dans chaque variable, afin d'identifier les colonnes problématiques et de réfléchir aux stratégies de traitement : suppression, imputation, ou autres méthodes adaptées.\n",
        "\n",
        "Nous allons afficher :\n",
        "- Le nombre de valeurs manquantes par colonne\n",
        "- Leur proportion en pourcentage\n",
        "- Un tri décroissant pour mieux visualiser les plus impactées\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4V0V3nX63lbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "4V0V3nX63lbf",
        "outputId": "891283dd-dcbd-4b3f-cef0-e8ed7f646d23"
      },
      "outputs": [],
      "source": [
        "# Nombre et taux (%) de valeurs manquantes\n",
        "nb_missing = X_train.isnull().sum()\n",
        "taux_missing = X_train.isnull().mean() * 100\n",
        "\n",
        "# DataFrame résumé\n",
        "missing_info = pd.DataFrame({\n",
        "    'Valeurs manquantes': nb_missing,\n",
        "    'Taux (%)': taux_missing.round(2)\n",
        "})\n",
        "\n",
        "# Filtrage des colonnes avec au moins une valeur manquante\n",
        "missing_info = missing_info[missing_info['Valeurs manquantes'] > 0]\n",
        "\n",
        "# Tri décroissant par taux\n",
        "missing_info = missing_info.sort_values(by='Taux (%)', ascending=True)\n",
        "\n",
        "# Affichage\n",
        "print(\"Résumé des valeurs manquantes :\")\n",
        "display(missing_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iWIENFh_4G70",
      "metadata": {
        "id": "iWIENFh_4G70"
      },
      "source": [
        "L'analyse révèle une proportion importante de variables avec des taux de valeurs manquantes élevés :\n",
        "\n",
        "- **Totalement ou quasi totalement manquantes** : `DEROG14` (100%), `DEROG13` (99.66%), `DEROG16` (99.15%)\n",
        "- **Taux très élevés (> 90%)** : `CARACT2`, `CARACT3`, `TYPBAT1`, `DEROG12`\n",
        "- **Taux intermédiaires (56-62%)** : variables météo et distances (`RRAB_VOR_MM_A`, `DISTANCE_311`, etc.)\n",
        "- **Taux modérés (20-40%)** : `RISK13`, `KAPITAL11`, `SURFACE8`, etc.\n",
        "- **Taux faibles (< 10%)** : `MEN_MAIS`, `RISK1`, `FRCH2`, etc.\n",
        "\n",
        "> **Remarques** :\n",
        "- Les variables avec plus de 90% de valeurs manquantes seront possiblement exclues ;\n",
        "- Certaines familles de variables (distances, météo) présentent une structure commune, ce qui permet d'envisager :\n",
        "  - une **imputation croisée** entre variables similaires,\n",
        "  - une **imputation par moyenne conditionnelle** selon des regroupements géographiques ou sociaux,\n",
        "  - ou une **réduction préalable** (moyenne, ACP) pour simplifier l'information.\n",
        "\n",
        "---\n",
        "\n",
        "Un grand nombre de caractéristiques présentant un volume important de valeurs manquantes, l'enjeu est d'identifier celles qui n'apporteront rien au modèle, afin de **réduire la dimensionnalité**.\n",
        "\n",
        "**Exemple** : les variables `DEROGnn`, avec plus de 99% de valeurs manquantes, seront probablement **inutiles pour prédire la fréquence du risque incendie**.\n",
        "\n",
        "Il conviendra d'**arbitrer variable par variable**, en fonction de la cible (`FREQ` ou `CN`).  \n",
        "Nous commencerons donc par **quantifier le taux de valeurs manquantes**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XxhzPi5TUM3B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XxhzPi5TUM3B",
        "outputId": "d8083a35-6a83-43af-c159-b96566a6b5b1"
      },
      "outputs": [],
      "source": [
        "# Analyse des valeurs uniques\n",
        "unique_values = X_train.nunique().sort_values(ascending=False)\n",
        "print(\"\\nNombre de valeurs uniques par colonne :\")\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EuVmWQdNVNIG",
      "metadata": {
        "id": "EuVmWQdNVNIG"
      },
      "source": [
        "### Analyse du nombre de modalités uniques par variable\n",
        "\n",
        "L'analyse du nombre de modalités uniques dans chaque variable permet d'anticiper leur traitement en modélisation (encodage, réduction, exclusion, etc.). Voici les principaux constats :\n",
        "\n",
        "- **Variables à forte cardinalité** :\n",
        "  - Certaines variables comme `ID` (383 610 valeurs uniques) ou `ANNEE_ASSURANCE` (>1000 modalités) présentent une **cardinalité extrêmement élevée**.\n",
        "  - Ces variables sont soit des identifiants à exclure (`ID`), soit potentiellement bruitées si elles ne peuvent être regroupées.\n",
        "\n",
        "- **Variables à cardinalité moyenne** :\n",
        "  - On trouve plusieurs variables avec **entre 10 et 100 modalités**, typiquement des scores (`KAPITAL*`, `SURFACE*`, `RISK*`, etc.), qui nécessiteront un **encodage adapté** (ordinal, binning, ou encodage fréquentiel).\n",
        "\n",
        "- **Variables à faible cardinalité ( ≤ 5 modalités)** :\n",
        "  - Très nombreuses, notamment dans les familles `DEROG`, `EQUIPEMENT`, `RISK`, `CARACT`, etc.\n",
        "  - Ces variables sont candidates à un **encodage one-hot**, ou pourront être exclues si elles sont redondantes ou peu informatives.\n",
        "\n",
        "- **Cas particuliers** :\n",
        "  - Des variables comme `DISTANCE_244` ou `IND_Y1_Y2` n'ont **qu'une seule modalité**, ce qui les rend inutiles pour l'apprentissage automatique (absence de variance).\n",
        "  - Les variables avec très peu de modalités et beaucoup de valeurs manquantes devront être analysées conjointement pour juger de leur utilité.\n",
        "\n",
        "> Cette vue apporte une première base pour :\n",
        "> - identifier les variables à exclure (trop ou pas assez variées),\n",
        "> - anticiper les stratégies d'encodage,\n",
        "> - planifier des regroupements ou des transformations.\n",
        "\n",
        "Il est important de noter que beaucoup de caractéristiques ont un nombre de valeurs uniques limité (par exemple 4) et sont de type `object`.\n",
        "\n",
        "Générons des tables de contingence afin d'évaluer la distribution des variables qualitatives.\n",
        "\n",
        "Ces tableaux permettront de :\n",
        "\n",
        "- Visualiser la **fréquence de chaque modalité** des variables catégorielles ;\n",
        "- Détecter les **modalités rares** ou déséquilibrées ;\n",
        "- Identifier des variables potentiellement peu informatives (modalité unique, déséquilibre extrême)\n",
        "- Préparer les décisions d'**encodage**, de **regroupement** ou d'**exclusion**\n",
        "\n",
        "Cette étape est nécessaire pour anticiper le comportement des variables catégorielles dans les modèles, notamment ceux sensibles aux déséquilibres ou à la cardinalité élevée.\n",
        "\n",
        "Pour les calculs des modèles, il nous faut convertir ces objets en catégories numériques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oz9B-JiPVymM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Oz9B-JiPVymM",
        "outputId": "2ca71af3-b04e-4169-d189-480ded917b00"
      },
      "outputs": [],
      "source": [
        "# Table de contingence pour les variables catégorielles\n",
        "# Sélection des colonnes catégorielles et tri alphabétique\n",
        "categorical_cols = sorted(X_train.select_dtypes(include=['object', 'category']).columns)\n",
        "\n",
        "# Affichage des tables de contingence\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nTable de contingence pour '{col}' :\")\n",
        "    print(X_train[col].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C5HB8JMPYPnL",
      "metadata": {
        "id": "C5HB8JMPYPnL"
      },
      "source": [
        "### Analyse quantitative des variables catégorielles\n",
        "\n",
        "#### 1. Nombre de modalités\n",
        "- Il apparaît une **grande hétérogénéité** dans le nombre de modalités :\n",
        "  - Certaines variables ont **plus de 100 modalités** (ex : `ANNEE_ASSURANCE` avec 1113 valeurs uniques).\n",
        "  - À l'inverse, un grand nombre de variables n'ont que **2 à 4 modalités**, comme `ADOSS`, `TYPERS`, `DEROG1`, etc.\n",
        "- Plus de **150 variables** possèdent **moins de 5 modalités**, ce qui les rend **faciles à encoder** (binaire, label encoding...).\n",
        "- Une large famille de variables telles que les `DISTANCE_xxx`, `RR_xxx`, `NBJ_xxx`, `FXI_xxx`, `TMM/TX/TN_xxx` ont souvent **4 classes ordinales** correspondant à des intervalles de valeurs (souvent par quartiles).\n",
        "\n",
        "#### 2. Distribution déséquilibrée\n",
        "- Beaucoup de variables présentent une **forte domination d'une modalité** :\n",
        "  - Exemple : `DEROG1` → 99.98% de \"N\", 0.02% de \"O\".\n",
        "  - `KAPITAL38` → 99.96% de \"N\".\n",
        "- D'autres variables comme `ACTIVIT2`, `VOCATION`, `CARACT4`, `MEN_1IND` sont déséquilibrées mais exploitables.\n",
        "\n",
        "#### 3. Présence de modalités rares\n",
        "- Certaines modalités apparaissent **moins de 10 fois** :\n",
        "  - Cela peut poser des problèmes de **sur-apprentissage** et **brouiller le signal** dans les modèles.\n",
        "  - Exemples : `DEROG14`, `DISTANCE_244`, certaines modalités de `MEN_PROP`.\n",
        "\n",
        "#### 4. Codages textuels ordonnés\n",
        "- De nombreuses variables sont issues de **discrétisation d'une variable continue** :\n",
        "  - Exemples de modalités : `01. <= 10`, `04. >= 40`, etc.\n",
        "  - Ces variables conservent une **structure ordinale** exploitable :\n",
        "    - **Encodage ordinal**\n",
        "    - Ou **reconversion en continu** via la borne médiane\n",
        "\n",
        "#### 5. Familles de variables cohérentes\n",
        "- Plusieurs familles homogènes peuvent être identifiées :\n",
        "  - `DISTANCE_xxx` : distances en classes (4-5 modalités).\n",
        "  - `NBJxxx_MM/MSOM/MMAX` : séries météo discrétisées.\n",
        "  - `PROPORTION_xx`, `MEN_xxx`, `KAPITALxx`, `DEROGxx` : codifications internes.\n",
        "- Ces regroupements permettent une **ingénierie de variables pertinente** :\n",
        "  - Moyennes ou scores internes\n",
        "  - Réduction de dimension (ACP)\n",
        "  - Agrégation par famille\n",
        "\n",
        "### Implications pour la modélisation\n",
        "\n",
        "- **Variables peu informatives** ou trop déséquilibrées :\n",
        "  - À écarter ou traiter spécifiquement.\n",
        "  - Ex : `DEROG14`, `IND_INC`, `DISTANCE_244` ;\n",
        "\n",
        "- **Variables ordinales exploitables** :\n",
        "  - Les variables comme `DISTANCE`, `METEO`, `PROPORTION`, etc., sont **structurées pour un encodage ordonné ou continu** ;\n",
        "\n",
        "- **Variables très dispersées** :\n",
        "  - Ex : `ANNEE_ASSURANCE`, `ZONE`\n",
        "  - Nécessitent un traitement spécifique : **groupement**, **encodage fréquence**, ou **embedding** selon le modèle.\n",
        "\n",
        "L'approche implémentée ici repose sur une stratégie **différenciée selon la nature et la complétude des variables**, avec les objectifs suivants :\n",
        "\n",
        "- **Préserver l'information existante** dans les colonnes peu affectées par des valeurs manquantes ;\n",
        "- **Limiter la distorsion** causée par l'imputation dans les colonnes fortement incomplètes ;\n",
        "- **Permettre au modèle d'apprendre** à partir du fait qu'une valeur était absente, en créant une modalité explicite (`'missing'`) pour les variables catégorielles très incomplètes ;\n",
        "\n",
        "#### Logique appliquée :\n",
        "- **Variables numériques :**\n",
        "  - Si moins de 30% de valeurs manquantes → imputation par la **médiane** (robuste aux outliers) ;\n",
        "  - Si plus de 30% de valeurs manquantes → imputation conservatrice par **zéro**, considérée comme une absence de valeur pertinente.\n",
        "\n",
        "- **Variables catégorielles :**\n",
        "  - Si moins de 30% de valeurs manquantes → imputation par la **valeur la plus fréquente** (mode);\n",
        "  - Si plus de 30% de valeurs manquantes → imputation par une valeur explicite `'missing'`, interprétable comme une modalité à part entière.\n",
        "\n",
        "Cette stratégie permet une **modélisation plus robuste**, tout en **évitant de supprimer prématurément des variables potentiellement informatives** malgré leur incomplétude.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2LtI39Qk8Oy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2LtI39Qk8Oy",
        "outputId": "1d03f603-8e35-40f6-c84d-e7a9e740c2e3"
      },
      "outputs": [],
      "source": [
        "print('ID' in X_train.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qYrrrKz1bTW6",
      "metadata": {
        "id": "qYrrrKz1bTW6"
      },
      "outputs": [],
      "source": [
        "# Séparation colonnes numériques / catégorielles\n",
        "num_cols = X_train.select_dtypes(include=['number']).columns\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# 1. Colonnes numériques\n",
        "# a. Colonnes avec peu de valeurs manquantes (<30%) → imputation par la médiane\n",
        "low_na_num = [col for col in num_cols if X_train[col].isna().mean() < 0.3]\n",
        "# b. Colonnes très incomplètes (>=30%) → imputation conservatrice par constante (0)\n",
        "high_na_num = [col for col in num_cols if X_train[col].isna().mean() >= 0.3]\n",
        "\n",
        "X_train[low_na_num] = SimpleImputer(strategy='median').fit_transform(X_train[low_na_num])\n",
        "X_train[high_na_num] = X_train[high_na_num].fillna(0)\n",
        "\n",
        "# 2. Colonnes catégorielles\n",
        "# a. Colonnes avec peu de NaN → imputation par la valeur la plus fréquente (mode)\n",
        "low_na_cat = [col for col in cat_cols if X_train[col].isna().mean() < 0.3]\n",
        "# b. Colonnes très incomplètes → imputation par 'missing' (valeur explicite)\n",
        "high_na_cat = [col for col in cat_cols if X_train[col].isna().mean() >= 0.3]\n",
        "\n",
        "X_train[low_na_cat] = SimpleImputer(strategy='most_frequent').fit_transform(X_train[low_na_cat])\n",
        "X_train[high_na_cat] = X_train[high_na_cat].fillna('missing')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jOG8CqnScON3",
      "metadata": {
        "id": "jOG8CqnScON3"
      },
      "source": [
        "Avant d'entraîner un modèle, il est essentiel d'écarter certaines variables qui **n'apportent pas d'information pertinente pour la prédiction**, ou qui pourraient **introduire du bruit ou des fuites de données** :\n",
        "\n",
        "- **`ID`** :  \n",
        "  - Identifiant unique de la ligne ou de l'observation ;\n",
        "  - **Sans signification statistique** : il n'est pas corrélé à la cible et risque d'être utilisé à tort par un modèle non régularisé comme un pseudo-prédicteur.\n",
        "  - Peut **induire du surapprentissage** si laissé dans les données.\n",
        "\n",
        "- **`ANNEE_ASSURANCE`** :  \n",
        "  - Variable qui peut être fortement **corrélée avec la date de déclaration**, de souscription ou avec le risque (biais temporel).\n",
        "  - Si la modélisation est censée être **générique et prédictive hors période**, cette colonne introduit un **risque de fuite temporelle**.\n",
        "  - À moins de construire un modèle par période ou de vouloir capter des effets saisonniers ou historiques, il est préférable de la supprimer.\n",
        "\n",
        "> Ces suppressions participent à la **robustesse** du modèle en limitant l'apprentissage sur des artefacts sans valeur prédictive réelle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vJ1qbZm7ceRs",
      "metadata": {
        "id": "vJ1qbZm7ceRs"
      },
      "outputs": [],
      "source": [
        "# Suppression des colonnes inutiles\n",
        "X_train = X_train.drop(['ID', 'ANNEE_ASSURANCE'], axis=1)\n",
        "\n",
        "# Encodage des variables catégoriques avec CountEncoder\n",
        "encoder = CountEncoder(cols=cat_cols)\n",
        "encoder.fit(X_train)\n",
        "X_train_enc = encoder.transform(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "voaAuyJRmpUy",
      "metadata": {
        "id": "voaAuyJRmpUy"
      },
      "source": [
        "### Graphe de distribution de la variable cible `CM` (coût moyen)\n",
        "\n",
        "Le graphe de distribution de la variable cible s'inscrit dans trois objectifs :\n",
        "- Détection de la distribution (loi normale, asymétrique, etc.) : Si la distribution est fortement asymétrique , cela peut indiquer qu'une transformation sera nécessaire pour améliorer les performances du modèle ;\n",
        "- Identification d'éventuelles valeurs extrêmes (*outliers*) : Les éléments isolés à l'extrémité du graphique révèlent des valeurs atypiques, qui peuvent biaiser l'apprentissage si non traitées ;\n",
        "- Ajustement du modèle et du choix de la métrique : Une distribution très déséquilibrée ou avec des zéros majoritaires peut influencer le choix de la fonction de perte ou des stratégies de rééchantillonnage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6s0Pm4FNl89P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "6s0Pm4FNl89P",
        "outputId": "5ef22ab0-2ec6-470a-e947-5f6f28b07f08"
      },
      "outputs": [],
      "source": [
        "# Graphe de distribution de y_train['CM']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_train['CM'], kde=True)\n",
        "plt.title('Distribution of CM')\n",
        "plt.xlabel('CM')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YIk0tgZGn-tL",
      "metadata": {
        "id": "YIk0tgZGn-tL"
      },
      "source": [
        "La distribution du coût moyen (CM) présente les caractéristiques suivantes :\n",
        "\n",
        "- Distribution extrêmement asymétrique à droite :\n",
        "  - Une immense majorité des valeurs de CM sont proches de zéro ;\n",
        "  - Quelques valeurs atteignent des niveaux très élevés (jusqu'à près de 500000), mais elles sont très rares.\n",
        "\n",
        "- Concentration des observations :\n",
        "  - Plus de 3 millions d'observations se situent dans l'intervalle 0 à quelques milliers d'euros, ce qui indique une forte concentration autour de petites valeurs ;\n",
        "  - Cela suggère que la majorité des sinistres ont un coût moyen faible.\n",
        "\n",
        "- Présence de valeurs extrêmes (outliers) :\n",
        "  - Quelques sinistres présentent un coût exceptionnellement élevé.\n",
        "  - Ces valeurs ont un poids potentiel important dans le calcul des métriques(RMSE, MAE…) et peuvent biaisser l'entraînement si non traitées.\n",
        "\n",
        "L'examen de la distribution brute de `CM` révèle une **forte asymétrie à droite** : la majorité des valeurs sont très faibles (proches de zéro), tandis qu'une minorité présente des montants exceptionnellement élevés.\n",
        "\n",
        "Ce comportement est typique des variables de type **coût de sinistres** ou **montants financiers**, souvent modélisées avec des **lois de Pareto**, **log-normales** ou **gamma**.\n",
        "\n",
        "Une transformation logarithmique `log(CM + 1)` permettrait de :\n",
        "  - Réduire l'effet des *outliers* ;\n",
        "  - Rapprocher la distribution d'une loi normale, facilitant la modélisation ;\n",
        "  - Améliorer la performance des modèles sensibles à la distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MzXOmI-LpLdO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "MzXOmI-LpLdO",
        "outputId": "4f2d42c5-1be8-4b61-a7a7-ac20c1da09e6"
      },
      "outputs": [],
      "source": [
        "# Transformation log(CM + 1) pour réduire la variance et la dissymétrie\n",
        "y_train['CM_log'] = np.log1p(y_train['CM'])  # log1p(x) = log(x + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_train['CM_log'], kde=True, bins=50)\n",
        "plt.title('Distribution de log(CM + 1)')\n",
        "plt.xlabel('log(CM + 1)')\n",
        "plt.ylabel('Fréquence')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qeB2lnK3p0rK",
      "metadata": {
        "id": "qeB2lnK3p0rK"
      },
      "source": [
        "Même après transformation `log(CM + 1)`, la distribution reste ultra-concentrée autour de zéro, ce qui n'est pas l'effet attendu d'un `log-transform`. Cela suggère fortement que **la majorité des valeurs de CM sont nulles ou quasi-nulles**.\n",
        "Vérifions cette hypothèse en deux temps :\n",
        "1. Vérifier la proportion de zéros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rvrWsBl2qdU4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvrWsBl2qdU4",
        "outputId": "0c234528-de22-44cf-99e9-9dae565f2430"
      },
      "outputs": [],
      "source": [
        "zero_ratio = (y_train['CM'] == 0).mean()\n",
        "print(f\"Proportion de zéros dans CM : {zero_ratio:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aEPMIgpyqkE8",
      "metadata": {
        "id": "aEPMIgpyqkE8"
      },
      "source": [
        "2. Affiche la distribution sans les zéros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gFmBtJ8rqkm6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "gFmBtJ8rqkm6",
        "outputId": "7036f1e3-298c-4c3e-ef17-1f8e47c2c5f9"
      },
      "outputs": [],
      "source": [
        "non_zero_cm = y_train[y_train['CM'] > 0]['CM']\n",
        "log_non_zero_cm = np.log1p(non_zero_cm)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(log_non_zero_cm, kde=True, bins=50)\n",
        "plt.title(\"Distribution de log(CM + 1) pour CM > 0\")\n",
        "plt.xlabel(\"log(CM + 1)\")\n",
        "plt.ylabel(\"Fréquence\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6TaQKK2NrDVe",
      "metadata": {
        "id": "6TaQKK2NrDVe"
      },
      "source": [
        "Plusieur éléments se dégagent de la transformation logarithmique sur les observations strictement positives (`CM > 0`) :\n",
        "\n",
        "1. **Réduction de l'asymétrie**  \n",
        "   La transformation logarithmique a permis de corriger l'asymétrie extrême initialement observée dans la distribution brute de `CM`. Elle :\n",
        "   - compresse l'effet des très grandes valeurs (*outliers*),\n",
        "   - met mieux en évidence la structure sous-jacente des montants courants.\n",
        "\n",
        "2. **Distribution toujours asymétrique à droite**  \n",
        "   Malgré la transformation, la distribution reste légèrement étalée vers la droite, ce qui reflète la nature typiquement déséquilibrée des montants de sinistres (une majorité de petits montants, quelques très gros).\n",
        "\n",
        "3. Le mode, qui constitue la valeur la plus fréquente dans une série de données, se situe autour de `log(CM + 1) ≈ 6`: Cela correspond à un coût de sinistre typique de `exp(6) - 1 ≈ 400 €`, valeur qui semble représenter un sinistre \"standard\" dans le jeu de données.\n",
        "\n",
        "Nous pouvons en déduire que cette transformation rend la variable cible plus adaptée à une régression, notamment linéaire, en :\n",
        "  - réduisant la variance,\n",
        "  - limitant le poids des valeurs extrêmes,\n",
        "  - facilitant la convergence des modèles.\n",
        "\n",
        "- Elle améliore également la **robustesse** des algorithmes face aux déséquilibres structurels inhérents aux données assurantielles.\n",
        "\n",
        "> Cette transformation est particulièrement utile dans une stratégie en deux étapes :  \n",
        "> 1. modèle binaire de prédiction de la survenue d'un sinistre (`CM > 0`),  \n",
        "> 2. régression sur `log(CM + 1)` conditionnée à `CM > 0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aOvLW_N7r0Rs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOvLW_N7r0Rs",
        "outputId": "a7c55d42-b52f-4aa2-d519-276b8508ee6e"
      },
      "outputs": [],
      "source": [
        "# 1. Création de la cible binaire (sinistre ou non)\n",
        "y_train_bin = (y_train['CM'] > 0).astype(int)\n",
        "\n",
        "# Séparation des données pour la phase de test\n",
        "X_tr, X_val, y_bin_tr, y_bin_val = train_test_split(X_train_enc, y_train_bin, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Étape 1 - Modèle de classification binaire : prédire si CM > 0\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_tr, y_bin_tr)\n",
        "\n",
        "# Prédictions de probabilité d’avoir un sinistre\n",
        "proba_sinistre = clf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Évaluation du modèle binaire\n",
        "auc = roc_auc_score(y_bin_val, proba_sinistre)\n",
        "print(f\"AUC (CM > 0): {auc:.4f}\")\n",
        "\n",
        "# 3. Étape 2 - Modèle de régression conditionnelle sur CM > 0\n",
        "# Filtrage sur les cas avec sinistre\n",
        "X_train_pos = X_train_enc[y_train['CM'] > 0]\n",
        "y_train_logcm = np.log1p(y_train['CM'][y_train['CM'] > 0])\n",
        "\n",
        "# Séparation pour validation\n",
        "X_tr2, X_val2, y_tr2, y_val2 = train_test_split(X_train_pos, y_train_logcm, test_size=0.2, random_state=42)\n",
        "\n",
        "# Régression sur log(CM + 1)\n",
        "reg = Ridge(alpha=1.0)\n",
        "reg.fit(X_tr2, y_tr2)\n",
        "\n",
        "# Prédictions\n",
        "y_pred_log = reg.predict(X_val2)\n",
        "rmse = mean_squared_error(y_val2, y_pred_log)\n",
        "print(f\"RMSE sur log(CM + 1): {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U_FRvBllw3Z0",
      "metadata": {
        "id": "U_FRvBllw3Z0"
      },
      "source": [
        "#### Analyse des performances du modèle\n",
        "\n",
        "#### 1. AUC (CM > 0) = **0.7371**\n",
        "\n",
        "L'AUC (*Area Under the Curve*) évalue la capacité du modèle à distinguer les observations où `CM > 0` (existence d'un sinistre) de celles où `CM = 0` (absence de sinistre).  \n",
        "Une AUC de **0.7371** indique une **bonne performance de discrimination** :  \n",
        "- Le modèle parvient à capter un signal pertinent.\n",
        "- Il est significativement meilleur que le hasard (AUC = 0.5).\n",
        "- Cela ouvre la voie à des **applications utiles pour le scoring ou la priorisation du risque**.\n",
        "\n",
        "#### 2. RMSE sur `log(CM + 1)` = **5.5240**\n",
        "\n",
        "Le RMSE est calculé ici sur la transformation logarithmique `log(CM + 1)`, souvent utilisée pour réduire l'impact des outliers dans une distribution très asymétrique.  \n",
        "Une erreur de 5.5240 en log se traduit par une erreur moyenne d’environ :\n",
        "\n",
        "$$\n",
        "e^{5.5240} - 1 \\approx 250 \\text{ €}\n",
        "$$\n",
        "\n",
        "Cela signifie qu'en moyenne, le modèle se trompe d'environ 250 € sur la prédiction du coût sinistre.  \n",
        "C'est une erreur significative si la plupart des sinistres sont faibles, mais raisonnable si la distribution comporte de nombreux sinistres très élevés.\n",
        "\n",
        "Ainsi :\n",
        "\n",
        "- Le modèle **discrimine correctement** la survenue d'un sinistre (AUC satisfaisante).\n",
        "- L'estimation du coût (RMSE) est perfectible, probablement à cause de la **présence d’outliers** importants.\n",
        "- Des améliorations possibles :\n",
        "  - Approche **en deux étapes** : classification (CM > 0) + régression sur les cas sinistrés.\n",
        "  - Utilisation de modèles **robustes aux valeurs extrêmes**.\n",
        "  - Test de **transformations alternatives** ou pondérations adaptées au domaine assurantiel.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
